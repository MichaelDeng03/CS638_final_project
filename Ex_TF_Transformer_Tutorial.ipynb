{"cells":[{"cell_type":"markdown","metadata":{"id":"CONTovn2cWtL"},"source":["\n","\n","# NLP From Scratch: Translation with a Transformer Network\n","\n","Tutorial adapted from [NLP From Scratch with PyTorch by Sean Robertson](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), and [Tensorflow Tutorial on Transformers](https://www.tensorflow.org/text/tutorials/transformer)\n","\n","In this tutorial, we will write our own classes and\n","functions to preprocess the data to do our NLP modeling tasks.\n","We hope after you complete this tutorial that you'll proceed to\n","learn how `torchtext` can handle much of this preprocessing for you.\n","\n","In this project we will be teaching a neural network to translate from\n","French to English.\n","\n","    [KEY: > input, = target, < output]\n","\n","    > il est en train de peindre un tableau .\n","    = he is painting a picture .\n","    < he is painting a picture .\n","\n","    > pourquoi ne pas essayer ce vin delicieux ?\n","    = why not try that delicious wine ?\n","    < why not try that delicious wine ?\n","\n","    > elle n est pas poete mais romanciere .\n","    = she is not a poet but a novelist .\n","    < she not not a poet but a novelist .\n","\n","    > vous etes trop maigre .\n","    = you re too skinny .\n","    < you re all alone .\n","\n","... to varying degrees of success.\n","\n","This is made possible by the simple but powerful idea of the [sequence\n","to sequence network](https://arxiv.org/abs/1409.3215)_, in which two neural\n","networks work together to transform one sequence to\n","another. An encoder network condenses an input sequence into a vector,\n","and a decoder network unfolds that vector into a new sequence.\n","\n","![Image of a Sequence to Sequence model](https://pytorch.org/tutorials/_images/seq2seq.png)\n","\n","To improve upon this model we'll use an [attention\n","mechanism](https://arxiv.org/abs/1409.0473)_, which lets the decoder\n","learn to focus over a specific range of the input sequence.\n","\n","**Recommended Reading on Sequence to Sequence networks:**\n","\n","-  [Learning Phrase Representations using RNN Encoder-Decoder for\n","   Statistical Machine Translation](https://arxiv.org/abs/1406.1078)_\n","-  [Sequence to Sequence Learning with Neural\n","   Networks](https://arxiv.org/abs/1409.3215)_\n","-  [Neural Machine Translation by Jointly Learning to Align and\n","   Translate](https://arxiv.org/abs/1409.0473)_\n","-  [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)_\n","\n","**Import Requirements**\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":1044,"status":"ok","timestamp":1714499643134,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"C0xm1axBYqUU"},"outputs":[],"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import re\n","import random\n","import numpy as np\n","\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"P4ePHhBBYlFv"},"source":["## Loading and preparing the data\n","\n","The data for this project is a set of many thousands of English to\n","French translation pairs.\n","Let's start by downloading the data to ``data/eng-fra.txt``.\n","This file is a tab separated list of translation pairs:\n","\n","`I am cold.    J'ai froid.`\n","\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1714499643427,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"7JrwQMaScgLw","outputId":"b06254ce-81a6-48d4-d7c9-cf155ff31251"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading file from https://download.pytorch.org/tutorial/data.zip\n","Extracting contents to .\n","Extraction completed.\n","\n","First 10 lines of the file:\n","Go.\tVa !\n","Run!\tCours !\n","Run!\tCourez !\n","Wow!\tÇa alors !\n","Fire!\tAu feu !\n","Help!\tÀ l'aide !\n","Jump.\tSaute.\n","Stop!\tÇa suffit !\n","Stop!\tStop !\n","Stop!\tArrête-toi !\n"]}],"source":["import requests\n","from zipfile import ZipFile\n","from io import BytesIO\n","\n","def download_and_unzip(url, extract_to='.'):\n","    \"\"\"\n","    Downloads a ZIP file from the given URL and unzips it to the given directory.\n","\n","    Parameters:\n","    url (str): The URL to download the ZIP file from.\n","    extract_to (str): The directory to extract the contents of the ZIP file to.\n","    \"\"\"\n","\n","    try:\n","        # Send a GET request to the URL\n","        print(f\"Downloading file from {url}\")\n","        response = requests.get(url)\n","        response.raise_for_status()\n","\n","        # Extract all the contents of the zip file in the directory 'extract_to'\n","        with ZipFile(BytesIO(response.content)) as zip_file:\n","            print(f\"Extracting contents to {extract_to}\")\n","            zip_file.extractall(path=extract_to)\n","            print(\"Extraction completed.\")\n","    except requests.exceptions.HTTPError as http_err:\n","        print(f\"HTTP error occurred: {http_err}\")  # Python 3.6\n","    except Exception as err:\n","        print(f\"An error occurred: {err}\")\n","\n","download_and_unzip('https://download.pytorch.org/tutorial/data.zip', '.')\n","\n","print('\\nFirst 10 lines of the file:')\n","!head data/eng-fra.txt\n"]},{"cell_type":"markdown","metadata":{"id":"g8EN7fCRYhE0"},"source":["We will now process the data into pairs of french and english sentences.\n","The full process for preparing the data is:\n"," - Read the text file, split each line into pairs (the pairs are tab separated)\n"," - Pre-process all sentences, and filter by length and content\n"," - Split sentences into list of words, create two dictionaries (for english and french).\n","\n","**Simplifications:**\n","   - The files are all in Unicode, we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation.\n","   - Since we want to train something quickly, we'll trim the data set to only relatively short (10 word maximum) and simple sentences (starting with \"I am\" or \"She is\" )."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5147,"status":"ok","timestamp":1714499648571,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"ntC_EJXDYeKQ","outputId":"31d1af98-2cc1-4b0f-a245-d0960183507c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading lines...\n","Processing lines...\n","Finished processing\n"]}],"source":["# Turn a Unicode string to plain ASCII, thanks to\n","# https://stackoverflow.com/a/518232/2809427\n","def unicode2ascii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","def preprocess_string(s):\n","    s = unicode2ascii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n","    return s.strip()\n","\n","MAX_LENGTH = 50\n","ENG_PREFIXES = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","# Filter pairs\n","def filter_pairs(pairs):\n","    subset = []\n","    for fr, en in pairs:\n","        if len(fr.split(' ')) > MAX_LENGTH:\n","            continue\n","        if len(en.split(' ')) > MAX_LENGTH:\n","            continue\n","        if not en.startswith(ENG_PREFIXES):\n","            continue\n","        subset.append((fr, en))\n","    return subset\n","\n","\n","# Read the data\n","def read_dataset(lang1, lang2, reverse=False):\n","\n","    # Read the file and split into lines\n","    print(\"Reading lines...\")\n","    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n","        read().strip().split('\\n')\n","\n","    # Split every line into pairs and normalize\n","    print(\"Processing lines...\")\n","    pairs = [[preprocess_string(s) for s in l.split('\\t')] for l in lines]\n","\n","    # Reverse pairs\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","\n","    # Filter pairs by length and content\n","    pairs = filter_pairs(pairs)\n","\n","    print(\"Finished processing\")\n","    return pairs\n","\n","corpus_pairs = read_dataset('eng', 'fra', reverse=True)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QjqQg6Ha-exm","executionInfo":{"status":"ok","timestamp":1714499648571,"user_tz":300,"elapsed":17,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"}},"outputId":"92b29db3-504c-49ee-95bc-af4db25e90b4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["13078"]},"metadata":{},"execution_count":27}],"source":["len(corpus_pairs)"]},{"cell_type":"markdown","metadata":{"id":"EFwSaNB8jF7s"},"source":["\n","<style>\n","td {\n","  text-align: center;\n","}\n","\n","th {\n","  text-align: center;\n","}\n","</style>"]},{"cell_type":"markdown","metadata":{"id":"awd8sx9kdiJR"},"source":["We'll need a unique index per word to use as the inputs and targets of\n","the networks later. To keep track of all this we will use a helper class\n","called ``Lang`` which has word → index (``word2index``) and index → word\n","(``index2word``) dictionaries, as well as a count of each word\n","``word2count`` which will be used to replace rare words later.\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1714499648571,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"x5UR3Wxtdflk"},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2, \"UNK\": 3}\n","        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n","        self.word2count = {}\n","        self.n_words = 4  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","    def tokenize(self, sentence, seq_len=None):\n","        # Add Start Of Sentence token\n","        token_seq_idx = [self.word2index[\"SOS\"]]\n","\n","        # Tokenize each word in sentence\n","        for tkn in sentence.split():\n","            token_seq_idx.append(self.word2index[tkn if tkn in self.word2index else \"UNK\"])\n","\n","        # Add End Of Sentence token\n","        token_seq_idx.append(self.word2index[\"EOS\"])\n","\n","        if seq_len is not None:\n","            if len(token_seq_idx) < seq_len:\n","                # Pad to desired lengh\n","                token_seq_idx += [self.word2index[\"PAD\"]] * (seq_len - len(token_seq_idx))\n","            else:\n","                # Trim sentence to length\n","                token_seq_idx = token_seq_idx[:seq_len]\n","\n","        return token_seq_idx\n","\n","    def list2sentence(self, seq_ids):\n","        return \" \".join([self.index2word[idx] for idx in seq_ids])\n","\n","\n","# print(\"Creating French and English dictionaries.\")\n","# fr_vocab = Lang('fr')\n","# en_vocab = Lang('en')\n","# for fr, en in corpus_pairs:\n","#     fr_vocab.addSentence(fr)\n","#     en_vocab.addSentence(en)\n","\n","# print(f\"French: {fr_vocab.n_words} words found.\")\n","# print(f\"English: {en_vocab.n_words} words found.\")"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1714499648572,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"UPtpu8h29lhg"},"outputs":[],"source":["def create_dataloaders(batch_size):\n","    # Create two huge tensors with all English and French sentences(+2 for START/END)\n","    n = len(corpus_pairs)\n","    french_seqs_ids = []\n","    english_seqs_ids = []\n","    for fr, en in corpus_pairs:\n","        french_seqs_ids.append(fr_vocab.tokenize(fr,MAX_LENGTH+2))\n","        english_seqs_ids.append(en_vocab.tokenize(en,MAX_LENGTH+2))\n","    french_seqs_ids = tf.convert_to_tensor(french_seqs_ids)\n","    english_seqs_ids = tf.convert_to_tensor(english_seqs_ids)\n","\n","    # Split into training and testing\n","    train_sample_mask = tf.random.uniform((n,)) > 0.3\n","    train_french_seqs_ids = french_seqs_ids[train_sample_mask]\n","    train_english_seqs_ids = english_seqs_ids[train_sample_mask]\n","    test_french_seqs_ids = french_seqs_ids[~train_sample_mask]\n","    test_english_seqs_ids = english_seqs_ids[~train_sample_mask]\n","\n","    # Create train dataset\n","    train_dataset = tf.data.Dataset.from_tensor_slices((train_french_seqs_ids, train_english_seqs_ids))\n","    train_dataset = train_dataset.shuffle(buffer_size=len(train_french_seqs_ids)).batch(batch_size)\n","\n","    # Create test dataset\n","    test_dataset = tf.data.Dataset.from_tensor_slices((test_french_seqs_ids, test_english_seqs_ids))\n","    test_dataset = test_dataset.batch(batch_size)\n","\n","    return train_dataset, test_dataset\n","\n","# # Test the dataloader\n","# train_dataloader, test_dataloader = create_dataloaders(32)\n","# for fr, en in train_dataloader.take(1):\n","#     print('Batch | fr =', fr.shape, '| en =', en.shape)\n","#     print('First sentence in French: ', fr[0].numpy())\n","#     print('First sentence in English:', en[0].numpy())\n","#     break\n"]},{"cell_type":"markdown","metadata":{"id":"a934948f7030"},"source":["# Neural machine translation with a Transformer and Keras"]},{"cell_type":"markdown","metadata":{"id":"TCg3ElKBUSBb"},"source":["This tutorial demonstrates how to create and train a [sequence-to-sequence](https://developers.google.com/machine-learning/glossary#sequence-to-sequence-task) [Transformer](https://developers.google.com/machine-learning/glossary#Transformer) model to translate French into English. The Transformer was originally proposed in [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017).\n","\n","\n","Transformers are deep neural networks that replace CNNs and RNNs with [self-attention](https://developers.google.com/machine-learning/glossary#self-attention). Self attention allows Transformers to easily transmit information across the input sequences.\n","\n","As explained in the [Google AI Blog post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html):\n","\n","> Neural networks for machine translation typically contain an encoder reading the input sentence and generating a representation of it. A decoder then generates the output sentence word by word while consulting the representation generated by the encoder. The Transformer starts by generating initial representations, or embeddings, for each word... Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations."]},{"cell_type":"markdown","metadata":{"id":"Fo1P7AN4lzdi"},"source":["<img src=\"https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif\" alt=\"Applying the Transformer to machine translation\">\n","\n","Figure 1: Applying the Transformer to machine translation. Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).\n"]},{"cell_type":"markdown","metadata":{"id":"FgH1s713EC2h"},"source":["## Why Transformers are significant\n","\n","- Transformers excel at modeling sequential data, such as natural language.\n","- Unlike the [recurrent neural networks (RNNs)](./text_generation.ipynb), Transformers are parallelizable. This makes them efficient on hardware like GPUs and TPUs. The main reasons is that Transformers replaced recurrence with attention, and computations can happen simultaneously. Layer outputs can be computed in parallel, instead of a series like an RNN.\n","- Unlike [RNNs](https://www.tensorflow.org/guide/keras/rnn) (like [seq2seq, 2014](https://arxiv.org/abs/1409.3215)) or [convolutional neural networks (CNNs)](https://www.tensorflow.org/tutorials/images/cnn) (for example, [ByteNet](https://arxiv.org/abs/1610.10099)), Transformers are able to capture distant or long-range contexts and dependencies in the data between distant positions in the input or output sequences. Thus, longer connections can be learned. Attention allows each location to have access to the entire input at each layer, while in RNNs and CNNs, the information needs to pass through many processing steps to move a long distance, which makes it harder to learn.\n","- Transformers make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, [StarCraft units](https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii)).\n","\n","<img src=\"https://www.tensorflow.org/images/tutorials/transformer/encoder_self_attention_distribution.png\" width=\"800\" alt=\"Encoder self-attention distribution for the word it from the 5th to the 6th layer of a Transformer trained on English-to-French translation\">\n","\n","Figure 3: The encoder self-attention distribution for the word “it” from the 5th to the 6th layer of a Transformer trained on English-to-French translation (one of eight attention heads). Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).\n"]},{"cell_type":"markdown","metadata":{"id":"9lXCXYFwECzV"},"source":["There's a lot going on inside a Transformer. The important things to remember are:\n","\n","1. It follows the same general pattern as a standard sequence-to-sequence model with an encoder and a decoder. The encoder processes the input sentence into a set of vector representations (one for each word), and the decoder uses the encoder's outputs to predict the target (ie, translated) sentence.\n","2. If you work through it step by step it will all make sense.\n","\n","<table>\n","<tr>\n","  <th colspan=1>The original Transformer diagram</th>\n","  <th colspan=1>A representation of a 4-layer Transformer</th>\n","</tr>\n","<tr>\n","  <td>\n","   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n","  </td>\n","  <td>\n","   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n","  </td>\n","</tr>\n","</table>\n","\n","Each of the components in these two diagrams will be explained next. Namely,\n","- Embedding and positional encoding layer\n","- Add & Norm layer\n","- Multi-Head Attention Layers\n","- Feed Forward Layers"]},{"cell_type":"markdown","metadata":{"id":"qiWx1FN6ECwM"},"source":["<table>\n","<tr>\n","<th colspan=1>The embedding and positional encoding layer</th>\n","<tr>\n","<tr>\n","<td>\n","<img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n","</td>\n","</tr>\n","</table>\n","\n","The inputs to both the encoder and decoder use the same embedding and positional encodings.\n","\n","First, given a sequence of tokens, both the input tokens (French) and target tokens (English) have to be converted into vectors using a `tf.keras.layers.Embedding` layer.\n","\n","Second, since attention layers see their input as an unordered set of vectors, it needs some way to identify word order. Otherwise, sentences like, `how are you`, `how you are`, `you how are`, and so on, would be indistinguishable. A Transformer adds a \"Positional Encoding\" to the embedding vectors. Positional Encodings are just vectors that uniquely identify word position. Also, ideally, nearby words should have similar position encodings.\n","\n","The original paper uses a set of sines and cosines with different frequencies (across the sequence) for calculating the positional encoding\n","\n","$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n","$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1714499648572,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"N1DJTAZc3C6_"},"outputs":[],"source":["@tf.function\n","def positional_encoding(length, depth):\n","  depth = depth/2\n","\n","  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","\n","  angle_rates = 1 / (10000**depths)         # (1, depth)\n","  angle_rads = positions * angle_rates      # (pos, depth)\n","\n","  pos_encoding = np.concatenate(\n","      [np.sin(angle_rads), np.cos(angle_rads)],\n","      axis=-1)\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","# pos_encoding = positional_encoding(length=2048, depth=512)\n","\n","# # Visualize Position Embeddings\n","# import matplotlib.pyplot as plt\n","# print(\"Position Encodings: (Max Position, Embedding Size) =\", pos_encoding.shape)\n","# plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n","# plt.ylabel('Depth')\n","# plt.xlabel('Position')\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OLSv69LNECtG"},"source":["To combine info about the word itself and the word location within the sequence, we create a WordPosEmbedding layer that looks-up a token’s embedding vector and adds the position vector. Since we are working with two different languages, we need to use two different token embeddings."]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1714499648572,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"838tmM1cm9cB"},"outputs":[],"source":["class WordPosEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, vocab_size, d_model):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n","    self.pos_encoding = tf.stop_gradient(positional_encoding(length=2048, depth=d_model))\n","\n","  def compute_mask(self, *args, **kwargs):\n","    return self.embedding.compute_mask(*args, **kwargs)\n","\n","  def call(self, x):\n","    length = x.shape[1]\n","    x = self.embedding(x)\n","    # This factor sets the relative scale of the embedding and positonal_encoding.\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x = x + self.pos_encoding[tf.newaxis, :length, :]\n","    return x\n","\n","\n","embed_fr = WordPosEmbedding(vocab_size=fr_vocab.n_words, d_model=512)\n","embed_en = WordPosEmbedding(vocab_size=en_vocab.n_words, d_model=512)\n"]},{"cell_type":"markdown","metadata":{"id":"sBvDYsDp64b7"},"source":["<table>\n","<tr>\n","<th colspan=2>Add and normalize</th>\n","<tr>\n","<tr>\n","<td>\n","<img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\"/>\n","</td>\n","</tr>\n","</table>\n","\n","These \"Add & Norm\" blocks are scattered throughout the model. Each one just implements a residual connection and followed by a `LayerNormalization` layer.\n","\n","The residual \"Add & Norm\" blocks are included so that training is efficient. The residual connection provides a direct path for the gradient (and ensures that vectors are **updated** by the attention layers instead of **replaced**), while the normalization maintains a reasonable scale for the outputs.\n"]},{"cell_type":"markdown","metadata":{"id":"lzqfWI1WJT2O"},"source":["Attention layers are used throughout the model. These are all identical except for how the attention is configured. Each one contains a `layers.MultiHeadAttention`, a `layers.LayerNormalization` and a `layers.Add`.\n","\n","<table>\n","<tr>\n","  <th colspan=2>The base attention layer</th>\n","<tr>\n","<tr>\n","  <td>\n","   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention.png\"/>\n","  </td>\n","</tr>\n","</table>"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1714499648572,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"VlOB_f2RJSK1"},"outputs":[],"source":["class BaseAttention(tf.keras.layers.Layer):\n","  def __init__(self, **kwargs):\n","    super().__init__()\n","    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n","    self.layernorm = tf.keras.layers.LayerNormalization()\n","    self.add = tf.keras.layers.Add()"]},{"cell_type":"markdown","metadata":{"id":"k-oLh95B-yys"},"source":["#### Attention refresher\n","\n","Before you get into the specifics of each usage, here is a quick refresher on how attention works.\n","\n","There are two inputs:\n","\n","1. The query sequence; the sequence being processed; the sequence doing the attending (bottom).\n","2. The context sequence; the sequence being attended to (left).\n","\n","The output has the same shape as the query-sequence.\n","\n","<table>\n","<tr>\n","  <th colspan=1>The base attention layer</th>\n","</tr>\n","<tr>\n","  <td>\n","   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention-new.png\"/>\n","  </td>\n","</tr>\n","</table>\n","\n","For an intuitive understanding of attention, the common comparison is that this operation is like a dictionary lookup.\n","A **fuzzy**, **differentiable**, **vectorized** dictionary lookup.\n","\n","Here's a regular python dictionary, with 3 keys and 3 values being passed a single query.\n","\n","```\n","d = {'color': 'blue', 'age': 22, 'type': 'pickup'}\n","result = d['color']\n","```\n","\n","- The `query`s is what you're trying to find.\n","- The `key`s what sort of information the dictionary has.\n","- The `value` is that information.\n","\n","When you look up a `query` in a regular dictionary, the dictionary finds the matching `key`, and returns its associated `value`.\n","The `query` either has a matching `key` or it doesn't.\n","You can imagine a **fuzzy** dictionary where the keys don't have to match perfectly.\n","If you looked up `d[\"species\"]` in the dictionary above, maybe you'd want it to return `\"pickup\"` since that's the best match for the query.\n","\n","An attention layer does a fuzzy lookup like this, but it's not just looking for the best key.\n","It combines the `values` based on how well the `query` matches each `key`.\n","\n","How does that work? In an attention layer the `query`, `key`, and `value` are each vectors.\n","Instead of doing a hash lookup the attention layer combines the `query` and `key` vectors to determine how well they match, the \"attention score\".\n","The layer returns the average across all the `values`, weighted by the \"attention scores\".\n","\n","Each location the query-sequence provides a `query` vector.\n","The context sequence acts as the dictionary. At each location in the context sequence provides a `key` and `value` vector.\n","The input vectors are not used directly, the `layers.MultiHeadAttention` layer includes `layers.Dense` layers to project the input vectors before using them.\n","\n","\n","There are three different types of attention layers used throughout the model. These are all identical except for how the attention is configured. Lets take a look at each one of them now."]},{"cell_type":"markdown","metadata":{"id":"j-q-BRcJA6-d"},"source":["<table>\n","<tr>\n","<th colspan=1>The cross attention layer</th>\n","<tr>\n","<tr>\n","<td>\n","<img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\"/>\n","</td>\n","</tr>\n","</table>\n","\n","At the literal center of the Transformer is the cross-attention layer. This layer connects the encoder and decoder.\n","It updates the decoder representations by attending to all encoder sequence. To implement this, you pass the target sequence `x` as the `query` and the `context` sequence as the `key/value` when calling the `mha` layer. Furthermore, since the queries can attend to the entire input sequence representation, obtained from the encoder, then no causal mask is applied.\n"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1714499648572,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"R42FrfjhBLgF"},"outputs":[],"source":["class CrossAttention(BaseAttention):\n","  def call(self, x, context):\n","    attn_output, attn_scores = self.mha(\n","        query=x,\n","        key=context,\n","        value=context,\n","        return_attention_scores=True)\n","\n","    # Cache the attention scores for plotting later.\n","    self.last_attn_scores = attn_scores\n","\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"9iJRQ1goECpr"},"source":["The caricature below shows how information flows through this layer. For simplicity the residual connections are not shown.\n","\n","The output length is the length of the `query` sequence, and not the length of the context `key/value` sequence.\n","The point is that each `query` location (english words in this example) can see all the `key/value` pairs in the context (input french words), but no information is exchanged between the queries.\n","\n","<table>\n","<tr>\n","  <th>Each query sees the whole context.</th>\n","</tr>\n","<tr>\n","  <td>\n","   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new.png\"/>\n","  </td>\n","</tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"JTS2AlTDECl3"},"source":["<table>\n","<tr>\n","  <th colspan=1>The global self attention layer</th>\n","<tr>\n","<tr>\n","  <td>\n","   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\"/>\n","  </td>\n","</tr>\n","</table>\n","\n","This layer is responsible for processing the context sequence (french sentence), and propagating information along its length.\n","\n","Since the context sequence is fixed while the translation is being generated, information is allowed to flow in both directions.\n","\n","Before Transformers, models commonly used RNNs or CNNs to do this task. However, RNNs and CNNs have their limitations.\n","\n","- The RNN allows information to flow all the way across the sequence, but it passes through many processing steps to get there (limiting gradient flow). These RNN steps have to be run sequentially and so the RNN is less able to take advantage of modern parallel devices.\n","- In the CNN each location can be processed in parallel, but it only provides a limited receptive field. The receptive field only grows linearly with the number of CNN layers,  You need to stack a number of Convolution layers to transmit information across the sequence ([Wavenet](https://arxiv.org/abs/1609.03499) reduces this problem by using dilated convolutions).\n","\n","The global self attention layer on the other hand lets every sequence element directly access every other sequence element, with only a few operations, and all the outputs can be computed in parallel.\n","\n","<table>\n","<tr>\n","  <th colspan=1>Bidirectional RNNs</th>\n","</tr>\n","<tr>\n","  <td>\n","   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN-bidirectional.png\"/>\n","  </td>\n","</tr>\n","<tr>\n","  <th colspan=1>CNNs</th>\n","</tr>\n","<tr>\n","  <td>\n","   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CNN.png\"/>\n","  </td>\n","</tr>\n","<tr>\n","  <th colspan=1>The global self attention layer</th>\n","</tr>\n","<tr>\n","  <td>\n","   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new.png\"/>\n","  </td>\n","</tr>\n","</table>\n","</table>\n","\n","To implement global self-attention you just need to pass the target sequence, `x`, as both the `query`, and `value` arguments to the `mha` layer:\n"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1714499648572,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"RNqoTpn1wB3i"},"outputs":[],"source":["class GlobalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(\n","        query=x,\n","        value=x,\n","        key=x)\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"1S8WWGb5DfD1"},"source":["<table>\n","<tr>\n","<th colspan=1>The causal self attention layer</th>\n","<tr>\n","<tr>\n","<td>\n","<img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png\"/>\n","</td>\n","</tr>\n","</table>\n","\n","This layer does a similar job as the global self attention layer, for the output sequence.\n","However, since we want to generate the output sequence word-by-word, the query sequence (ie, representing the english translation) can only attend to the previous (already generated) words: the models are \"causal\".\n","\n","A causal model is efficient in two ways:\n","\n","1. During training, we can feed the ground truth translation to the decoder input, and have it predict the very next token at all locations. This lets you compute loss for every location in the output sequence while executing the model just once.\n","2. During inference, for each new token generated you only need to calculate its outputs, the outputs for the previous sequence elements can be reused.\n","\n","Causal attention is accomplished using a causal mask, which ensures that each location only has access to the locations that come before it:\n","\n","<table>\n","<tr>\n","  <th colspan=1>The causal self attention layer</th>\n","<tr>\n","<tr>\n","  <td>\n","   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\"/>\n","  </td>\n","</tr>\n","  <td>\n","   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new.png\"/>\n","  </td>\n","</tr>\n","</table>\n","</table>\n","\n","\n","To build a causal self attention layer, you need to use an appropriate mask when computing the attention scores and summing the attention `value`s.\n","\n","This is taken care of automatically if you pass `use_causal_mask = True` to the `MultiHeadAttention` layer when you call it:\n","\n","\n","\n"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1714499648573,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"HWdnAo46phwD"},"outputs":[],"source":["class CausalSelfAttention(BaseAttention):\n","  def call(self, x):\n","    attn_output = self.mha(\n","        query=x,\n","        value=x,\n","        key=x,\n","        use_causal_mask = True)\n","\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"EuYXzMY6De_N"},"source":["<table>\n","<tr>\n","  <th colspan=1>The feed forward network</th>\n","<tr>\n","<tr>\n","  <td>\n","   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\"/>\n","  </td>\n","</tr>\n","</table>\n","\n","The transformer also includes point-wise feed-forward networks, which process each token independently (no interactions between words), in both the encoder and decoder.\n"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1714499648573,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"vapVWG27rZ6f"},"outputs":[],"source":["class FeedForward(tf.keras.layers.Layer):\n","  def __init__(self, d_model, dff, dropout_rate=0.1):\n","    super().__init__()\n","    self.seq = tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),\n","      tf.keras.layers.Dense(d_model),\n","      tf.keras.layers.Dropout(dropout_rate)\n","    ])\n","    self.add = tf.keras.layers.Add()\n","    self.layer_norm = tf.keras.layers.LayerNormalization()\n","\n","  def call(self, x):\n","    x = self.add([x, self.seq(x)])\n","    x = self.layer_norm(x)\n","\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"bCBLECsuDe8F"},"source":["### The Encoder and Decoder\n","\n","Now, that we know how each component of a Transformer model work, lets put them all together.\n","\n","The encoder contains a `WordPosEmbedding` layer at the input and a stack of `N` encoder layers. Each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer.\n","\n","<table>\n","<tr>\n","  <td>\n","   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Encoder.png\"/>\n","  </td>\n","</tr>\n","</table>\n"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1714499648573,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"ncyS-Ms3i2x_","outputId":"715f4381-31a1-4df7-cc2f-4a1c6791d8c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 1, 512)\n","(1, 1, 512)\n"]}],"source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.self_attention = GlobalSelfAttention(\n","        num_heads=num_heads,\n","        key_dim=d_model,\n","        dropout=dropout_rate)\n","\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x):\n","    x = self.self_attention(x)\n","    x = self.ffn(x)\n","\n","    return x\n","\n","sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n","\n","print(fr_tkn_seq.shape)\n","print(sample_encoder_layer(fr_tkn_seq).shape)"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714499648573,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"cjxMnMGZL4A1"},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads,\n","               dff, vocab_size, dropout_rate=0.1):\n","    super().__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.pos_embedding = WordPosEmbedding(\n","        vocab_size=vocab_size, d_model=d_model)\n","\n","    self.enc_layers = [\n","        EncoderLayer(d_model=d_model,\n","                     num_heads=num_heads,\n","                     dff=dff,\n","                     dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","\n","  def call(self, x):\n","    # `x` is token-IDs shape: (batch, seq_len)\n","    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n","\n","    # Add dropout.\n","    x = self.dropout(x)\n","\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x)\n","\n","    return x  # Shape `(batch_size, seq_len, d_model)`."]},{"cell_type":"markdown","metadata":{"id":"ShKxTr7quT-R"},"source":["The decoder's stack starts with `WordPosEmbedding`, followed by a series of `DecoderLayer`, containing a `CausalSelfAttention`, a `CrossAttention`, and a `FeedForward` layer.\n","\n","<table>\n","<tr>\n","  <td>\n","   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png\"/>\n","  </td>\n","</tr>\n","</table>"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714499648573,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"HtWkj8OluBiA"},"outputs":[],"source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self,\n","               *,\n","               d_model,\n","               num_heads,\n","               dff,\n","               dropout_rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.causal_self_attention = CausalSelfAttention(\n","        num_heads=num_heads,\n","        key_dim=d_model,\n","        dropout=dropout_rate)\n","\n","    self.cross_attention = CrossAttention(\n","        num_heads=num_heads,\n","        key_dim=d_model,\n","        dropout=dropout_rate)\n","\n","    self.ffn = FeedForward(d_model, dff)\n","\n","  def call(self, x, context):\n","    x = self.causal_self_attention(x=x)\n","    x = self.cross_attention(x=x, context=context)\n","\n","    # Cache the last attention scores for plotting later\n","    self.last_attn_scores = self.cross_attention.last_attn_scores\n","\n","    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n","\n","    return x"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714499648573,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"jwIb6gKENXDo"},"outputs":[],"source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n","               dropout_rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.pos_embedding = WordPosEmbedding(vocab_size=vocab_size,\n","                                             d_model=d_model)\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","    self.dec_layers = [\n","        DecoderLayer(d_model=d_model, num_heads=num_heads,\n","                     dff=dff, dropout_rate=dropout_rate)\n","        for _ in range(num_layers)]\n","\n","    self.last_attn_scores = None\n","\n","  def call(self, x, context):\n","    # `x` is token-IDs shape (batch, target_seq_len)\n","    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n","\n","    x = self.dropout(x)\n","\n","    for i in range(self.num_layers):\n","      x  = self.dec_layers[i](x, context)\n","\n","    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n","\n","    # The shape of x is (batch_size, target_seq_len, d_model).\n","    return x"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714499648574,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"PxDfaJ2RN3IQ"},"outputs":[],"source":[]},{"cell_type":"code","source":["class EarlyStopping(tf.keras.callbacks.Callback):\n","    def __init__(self, patience=0, min_delta=0):\n","        super(EarlyStopping, self).__init__()\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.best_loss = None\n","        self.counter = 0\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        val_loss = logs.get('val_loss')\n","        if val_loss is None:\n","            raise ValueError(\"Early stopping requires validation loss.\")\n","\n","        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n","            self.best_loss = val_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.model.stop_training = True"],"metadata":{"id":"Lr9nZATwiEop","executionInfo":{"status":"ok","timestamp":1714499648574,"user_tz":300,"elapsed":10,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714499648574,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"PED3bIpOYkBu"},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","  def __init__(self, *, num_layers, d_model, num_heads, dff,\n","               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n","    super().__init__()\n","    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n","                           num_heads=num_heads, dff=dff,\n","                           vocab_size=input_vocab_size,\n","                           dropout_rate=dropout_rate)\n","\n","    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n","                           num_heads=num_heads, dff=dff,\n","                           vocab_size=target_vocab_size,\n","                           dropout_rate=dropout_rate)\n","\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","  def call(self, inputs):\n","    # If want to use keras `.fit` you must pass all your inputs as one\n","    x, context = inputs\n","\n","    context = self.encoder(context)  # (batch_size, context_len, d_model)\n","\n","    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n","\n","    # Final linear layer output.\n","    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n","\n","    # Return the final output and the attention weights.\n","    return logits\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pQhKvs27xR86"},"source":["## Evaluation\n","\n","Evaluation is mostly the same as training, but there are no targets so\n","we simply feed the decoder's predictions back to itself for each step.\n","Every time it predicts a word we add it to the output string, and if it\n","predicts the EOS token we stop there.\n","\n","We can evaluate random sentences from the training set and print out the\n","input, target, and output to make some subjective quality judgements.\n"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714499648574,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"Ef2qxG28wdE8"},"outputs":[],"source":["import tqdm\n","\n","def evaluate(transformer, fr_sentence):\n","  # The French sentence is tokenized and converted to a batch of B=1\n","  french_tkns = tf.expand_dims(tf.convert_to_tensor([fr_vocab.word2index[word] for word in fr_sentence.split()], dtype=tf.int32), axis=0)\n","  # First, the sentence to be translated is encoded using the transformer encoder.\n","  french_feats = transformer.encoder(french_tkns)\n","\n","  # The translation sentence is initialized with SOS token\n","  decoded_tkns = tf.convert_to_tensor([[en_vocab.word2index['SOS']]], dtype=tf.int32)\n","\n","  # We'll keep track of the predicted logits in order to compute the perplexity\n","  pred_logits = []\n","\n","  # Then, we evaluate the decoder, to generate the next words in the translation, one word at a time.\n","  for _ in range(MAX_LENGTH - 1):\n","\n","      next_pred_feat = transformer.decoder(decoded_tkns,french_feats)[:, -1]\n","      next_pred_logit = transformer.final_layer(next_pred_feat)\n","      next_pred = tf.expand_dims(tf.argmax(next_pred_logit, axis=1, output_type=tf.int32),axis=0)\n","      pred_logits.append(next_pred_logit)\n","      if next_pred.numpy()[0] == en_vocab.word2index['EOS']:\n","          break\n","      decoded_tkns = tf.concat([decoded_tkns, next_pred], axis=1)\n","\n","  decoded_tkns = tf.squeeze(decoded_tkns, axis=0)  # Squeeze batch dimension\n","  translation_words = [en_vocab.index2word[idx.numpy()] for idx in decoded_tkns[1:]]\n","  pred_logits = tf.concat(pred_logits, axis=0)\n","  return translation_words, decoded_tkns, pred_logits\n","\n","\n","def evaluate_one_epoch(transformer, n=100):\n","  criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","  loss, perplexity = 0.0, 0.0\n","  for i in tqdm.tqdm(range(n), desc='[EVAL]'):\n","    for fr_tkn, en_tkn in test_dataloader.shuffle(10).take(1):\n","      en_tkn = en_tkn[0].numpy()\n","      fr_tkn = fr_tkn[0].numpy()\n","      fr = fr_vocab.list2sentence(fr_tkn[en_tkn>2])\n","      _, _, pred_logits = evaluate(transformer, fr)\n","      l = criterion(en_tkn[1:1 + len(pred_logits)], pred_logits).numpy()\n","      loss += l\n","      perplexity += tf.exp(l)\n","\n","  return loss / n, perplexity / n\n","\n","def translate_randomly(transformer, n=3):\n","  for fr_tkn, en_tkn in test_dataloader.shuffle(10).take(n):\n","    en_tkn = en_tkn[0].numpy()\n","    fr_tkn = fr_tkn[0].numpy()\n","\n","    en = en_vocab.list2sentence(en_tkn[en_tkn>2])\n","    fr = fr_vocab.list2sentence(fr_tkn[en_tkn>2])\n","\n","    print('>', fr)\n","    print('=', en)\n","    output_sentence, _, _ = evaluate(transformer, fr)\n","    print('<', ' '.join(output_sentence))\n","    print('')"]},{"cell_type":"markdown","metadata":{"id":"DBp1ahsPECRk"},"source":["## Training\n","\n","To train, we use our typical training loop to optimize all weights of the model by gradient descent.\n","The transformer model receives as input batches of French and English sentence pairs. The encoder processes the French sentence globally, and the decoder processes the English sentence causally (only looking at previous tokens) while simultaneously attending to the encoder output.\n","The transformer finally outputs a prediction if the next word, at each point in the translation.\n","The model is trained to optimize the CrossEntropy loss (over the English dictionary) using the Adam optimizer."]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714501217401,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"ib5WX314kI-3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","def show_plot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)\n","\n","def train_epoch(train_dataloader, transformer, optimizer, criterion):\n","  total_loss = 0\n","  for fr_tensor, en_tensor in train_dataloader:\n","    en_past = en_tensor[:, :-1]\n","    en_target = en_tensor[:, 1:]\n","    # print(en_past.shape, en_target.shape, fr_tensor.shape)\n","\n","    with tf.GradientTape() as tape:\n","        preds = transformer((en_past,fr_tensor))\n","        loss = criterion(\n","            tf.reshape(en_target,[-1]),\n","            tf.reshape(preds,[-1, preds.shape[-1]]),\n","            )\n","    gradients = tape.gradient(loss, transformer.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","    total_loss += loss.numpy()\n","\n","\n","  return total_loss/ len(list(train_dataloader))\n","\n","class CustomEarlyStopping:\n","    def __init__(self, patience=0, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.best_loss = None\n","        self.counter = 0\n","\n","    def __call__(self, epoch, logs=None):\n","        val_loss = logs.get('val_loss')\n","        if val_loss is None:\n","            raise ValueError(\"Early stopping requires validation loss.\")\n","\n","        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n","            self.best_loss = val_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","\n","        return False\n","\n","def train(train_dataset, transformer, optimizer, n_epochs, print_every=2, plot_every=1):\n","    plot_losses = []\n","    criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","    # Create the custom early stopping callback\n","    early_stopping = CustomEarlyStopping(patience=5)\n","\n","    for epoch in tqdm.tqdm(range(n_epochs), desc='[TRAIN]'):\n","        loss = train_epoch(train_dataloader, transformer, optimizer, criterion)\n","\n","        if epoch % print_every == 0:\n","            te_loss, te_perplexity = evaluate_one_epoch(transformer)\n","            print(f'[Epoch={epoch}/{n_epochs}] Training Loss={loss:.4f}. Test Loss = {te_loss:.4f}. Test Perplexity = {te_perplexity:.2f}')\n","            translate_randomly(transformer, n=3)\n","\n","            # Check if early stopping criterion is met\n","            if early_stopping(epoch, logs={'val_loss': te_loss}):\n","                print(f\"Early stopping triggered at epoch {epoch}\")\n","                break\n","\n","        if epoch % plot_every == 0:\n","            plot_losses.append(loss)\n","\n","    show_plot(plot_losses)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mzyo6KDfVyhl","outputId":"88e063bd-d997-496a-8a9a-39efdc3096f6"},"outputs":[{"output_type":"stream","name":"stderr","text":["[TRAIN]:   0%|          | 0/30 [00:00<?, ?it/s]\n","[EVAL]:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n","[EVAL]:   1%|          | 1/100 [00:02<04:50,  2.94s/it]\u001b[A\n","[EVAL]:   2%|▏         | 2/100 [00:05<04:41,  2.87s/it]\u001b[A\n","[EVAL]:   3%|▎         | 3/100 [00:08<04:35,  2.84s/it]\u001b[A\n","[EVAL]:   4%|▍         | 4/100 [00:11<04:29,  2.81s/it]\u001b[A\n","[EVAL]:   5%|▌         | 5/100 [00:14<04:25,  2.80s/it]\u001b[A\n","[EVAL]:   6%|▌         | 6/100 [00:16<04:24,  2.82s/it]\u001b[A\n","[EVAL]:   7%|▋         | 7/100 [00:19<04:21,  2.81s/it]\u001b[A\n","[EVAL]:   8%|▊         | 8/100 [00:22<04:16,  2.79s/it]\u001b[A\n","[EVAL]:   9%|▉         | 9/100 [00:25<04:14,  2.79s/it]\u001b[A\n","[EVAL]:  10%|█         | 10/100 [00:28<04:11,  2.80s/it]\u001b[A\n","[EVAL]:  11%|█         | 11/100 [00:30<04:11,  2.82s/it]\u001b[A\n","[EVAL]:  12%|█▏        | 12/100 [00:33<04:08,  2.82s/it]\u001b[A\n","[EVAL]:  13%|█▎        | 13/100 [00:36<04:05,  2.83s/it]\u001b[A\n","[EVAL]:  14%|█▍        | 14/100 [00:39<04:05,  2.85s/it]\u001b[A\n","[EVAL]:  15%|█▌        | 15/100 [00:42<03:59,  2.82s/it]\u001b[A\n","[EVAL]:  16%|█▌        | 16/100 [00:45<03:55,  2.81s/it]\u001b[A\n","[EVAL]:  17%|█▋        | 17/100 [00:48<03:56,  2.84s/it]\u001b[A\n","[EVAL]:  18%|█▊        | 18/100 [00:51<03:59,  2.92s/it]\u001b[A\n","[EVAL]:  19%|█▉        | 19/100 [00:54<03:57,  2.93s/it]\u001b[A\n","[EVAL]:  20%|██        | 20/100 [00:56<03:52,  2.91s/it]\u001b[A\n","[EVAL]:  21%|██        | 21/100 [00:59<03:48,  2.89s/it]\u001b[A\n","[EVAL]:  22%|██▏       | 22/100 [01:02<03:51,  2.97s/it]\u001b[A\n","[EVAL]:  23%|██▎       | 23/100 [01:05<03:45,  2.93s/it]\u001b[A\n","[EVAL]:  24%|██▍       | 24/100 [01:08<03:39,  2.89s/it]\u001b[A\n","[EVAL]:  25%|██▌       | 25/100 [01:11<03:33,  2.85s/it]\u001b[A\n","[EVAL]:  26%|██▌       | 26/100 [01:14<03:30,  2.84s/it]\u001b[A\n","[EVAL]:  27%|██▋       | 27/100 [01:16<03:26,  2.82s/it]\u001b[A\n","[EVAL]:  28%|██▊       | 28/100 [01:19<03:22,  2.82s/it]\u001b[A\n","[EVAL]:  29%|██▉       | 29/100 [01:22<03:19,  2.80s/it]\u001b[A\n","[EVAL]:  30%|███       | 30/100 [01:25<03:16,  2.81s/it]\u001b[A\n","[EVAL]:  31%|███       | 31/100 [01:28<03:14,  2.81s/it]\u001b[A\n","[EVAL]:  32%|███▏      | 32/100 [01:30<03:11,  2.82s/it]\u001b[A\n","[EVAL]:  33%|███▎      | 33/100 [01:33<03:09,  2.83s/it]\u001b[A\n","[EVAL]:  34%|███▍      | 34/100 [01:36<03:06,  2.83s/it]\u001b[A\n","[EVAL]:  35%|███▌      | 35/100 [01:39<03:01,  2.80s/it]\u001b[A\n","[EVAL]:  36%|███▌      | 36/100 [01:42<02:57,  2.78s/it]\u001b[A\n","[EVAL]:  37%|███▋      | 37/100 [01:44<02:54,  2.77s/it]\u001b[A\n","[EVAL]:  38%|███▊      | 38/100 [01:47<02:51,  2.77s/it]\u001b[A\n","[EVAL]:  39%|███▉      | 39/100 [01:50<02:48,  2.77s/it]\u001b[A\n","[EVAL]:  40%|████      | 40/100 [01:53<02:45,  2.76s/it]\u001b[A\n","[EVAL]:  41%|████      | 41/100 [01:55<02:41,  2.74s/it]\u001b[A\n","[EVAL]:  42%|████▏     | 42/100 [01:58<02:40,  2.76s/it]\u001b[A\n","[EVAL]:  43%|████▎     | 43/100 [02:01<02:37,  2.76s/it]\u001b[A\n","[EVAL]:  44%|████▍     | 44/100 [02:04<02:35,  2.77s/it]\u001b[A\n","[EVAL]:  45%|████▌     | 45/100 [02:06<02:31,  2.76s/it]\u001b[A\n","[EVAL]:  46%|████▌     | 46/100 [02:09<02:28,  2.76s/it]\u001b[A\n","[EVAL]:  47%|████▋     | 47/100 [02:12<02:26,  2.77s/it]\u001b[A\n","[EVAL]:  48%|████▊     | 48/100 [02:15<02:23,  2.77s/it]\u001b[A\n","[EVAL]:  49%|████▉     | 49/100 [02:18<02:22,  2.78s/it]\u001b[A\n","[EVAL]:  50%|█████     | 50/100 [02:20<02:19,  2.79s/it]\u001b[A\n","[EVAL]:  51%|█████     | 51/100 [02:23<02:15,  2.77s/it]\u001b[A\n","[EVAL]:  52%|█████▏    | 52/100 [02:26<02:12,  2.75s/it]\u001b[A\n","[EVAL]:  53%|█████▎    | 53/100 [02:28<02:08,  2.74s/it]\u001b[A\n","[EVAL]:  54%|█████▍    | 54/100 [02:31<02:06,  2.75s/it]\u001b[A\n","[EVAL]:  55%|█████▌    | 55/100 [02:34<02:05,  2.78s/it]\u001b[A\n","[EVAL]:  56%|█████▌    | 56/100 [02:37<02:02,  2.78s/it]\u001b[A\n","[EVAL]:  57%|█████▋    | 57/100 [02:40<01:59,  2.77s/it]\u001b[A\n","[EVAL]:  58%|█████▊    | 58/100 [02:42<01:56,  2.78s/it]\u001b[A\n","[EVAL]:  59%|█████▉    | 59/100 [02:45<01:53,  2.77s/it]\u001b[A\n","[EVAL]:  60%|██████    | 60/100 [02:48<01:50,  2.77s/it]\u001b[A\n","[EVAL]:  61%|██████    | 61/100 [02:51<01:47,  2.75s/it]\u001b[A\n","[EVAL]:  62%|██████▏   | 62/100 [02:53<01:45,  2.77s/it]\u001b[A\n","[EVAL]:  63%|██████▎   | 63/100 [02:56<01:42,  2.77s/it]\u001b[A\n","[EVAL]:  64%|██████▍   | 64/100 [02:59<01:39,  2.77s/it]\u001b[A\n","[EVAL]:  65%|██████▌   | 65/100 [03:02<01:36,  2.77s/it]\u001b[A\n","[EVAL]:  66%|██████▌   | 66/100 [03:05<01:34,  2.78s/it]\u001b[A\n","[EVAL]:  67%|██████▋   | 67/100 [03:07<01:31,  2.78s/it]\u001b[A\n","[EVAL]:  68%|██████▊   | 68/100 [03:10<01:28,  2.76s/it]\u001b[A\n","[EVAL]:  69%|██████▉   | 69/100 [03:13<01:25,  2.77s/it]\u001b[A\n","[EVAL]:  70%|███████   | 70/100 [03:16<01:22,  2.76s/it]\u001b[A\n","[EVAL]:  71%|███████   | 71/100 [03:18<01:19,  2.76s/it]\u001b[A\n","[EVAL]:  72%|███████▏  | 72/100 [03:21<01:17,  2.76s/it]\u001b[A\n","[EVAL]:  73%|███████▎  | 73/100 [03:24<01:14,  2.76s/it]\u001b[A\n","[EVAL]:  74%|███████▍  | 74/100 [03:27<01:11,  2.75s/it]\u001b[A\n","[EVAL]:  75%|███████▌  | 75/100 [03:29<01:09,  2.76s/it]\u001b[A\n","[EVAL]:  76%|███████▌  | 76/100 [03:32<01:06,  2.76s/it]\u001b[A\n","[EVAL]:  77%|███████▋  | 77/100 [03:35<01:04,  2.80s/it]\u001b[A\n","[EVAL]:  78%|███████▊  | 78/100 [03:38<01:01,  2.79s/it]\u001b[A\n","[EVAL]:  79%|███████▉  | 79/100 [03:41<00:58,  2.78s/it]\u001b[A\n","[EVAL]:  80%|████████  | 80/100 [03:43<00:55,  2.76s/it]\u001b[A\n","[EVAL]:  81%|████████  | 81/100 [03:46<00:52,  2.74s/it]\u001b[A\n","[EVAL]:  82%|████████▏ | 82/100 [03:49<00:49,  2.73s/it]\u001b[A\n","[EVAL]:  83%|████████▎ | 83/100 [03:51<00:46,  2.74s/it]\u001b[A\n","[EVAL]:  84%|████████▍ | 84/100 [03:54<00:43,  2.73s/it]\u001b[A\n","[EVAL]:  85%|████████▌ | 85/100 [03:57<00:41,  2.73s/it]\u001b[A\n","[EVAL]:  86%|████████▌ | 86/100 [04:00<00:38,  2.74s/it]\u001b[A\n","[EVAL]:  87%|████████▋ | 87/100 [04:02<00:35,  2.75s/it]\u001b[A\n","[EVAL]:  88%|████████▊ | 88/100 [04:05<00:33,  2.76s/it]\u001b[A\n","[EVAL]:  89%|████████▉ | 89/100 [04:08<00:30,  2.76s/it]\u001b[A\n","[EVAL]:  90%|█████████ | 90/100 [04:11<00:27,  2.76s/it]\u001b[A\n","[EVAL]:  91%|█████████ | 91/100 [04:14<00:24,  2.76s/it]\u001b[A\n","[EVAL]:  92%|█████████▏| 92/100 [04:16<00:22,  2.75s/it]\u001b[A\n","[EVAL]:  93%|█████████▎| 93/100 [04:19<00:19,  2.74s/it]\u001b[A\n","[EVAL]:  94%|█████████▍| 94/100 [04:22<00:16,  2.74s/it]\u001b[A\n","[EVAL]:  95%|█████████▌| 95/100 [04:25<00:13,  2.76s/it]\u001b[A\n","[EVAL]:  96%|█████████▌| 96/100 [04:27<00:11,  2.75s/it]\u001b[A\n","[EVAL]:  97%|█████████▋| 97/100 [04:30<00:08,  2.75s/it]\u001b[A\n","[EVAL]:  98%|█████████▊| 98/100 [04:33<00:05,  2.75s/it]\u001b[A\n","[EVAL]:  99%|█████████▉| 99/100 [04:36<00:02,  2.79s/it]\u001b[A\n","[EVAL]: 100%|██████████| 100/100 [04:38<00:00,  2.79s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch=0/30] Training Loss=1.3483. Test Loss = 0.6898. Test Perplexity = 2.00\n","> vous etes jeune\n","= you re young\n","< PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n","\n","> je suis bon en chant\n","= i m good at singing\n","< PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n","\n","> nous sommes des\n","= we re prisoners\n"]},{"output_type":"stream","name":"stderr","text":["\r[TRAIN]:   3%|▎         | 1/30 [05:31<2:39:59, 331.02s/it]"]},{"output_type":"stream","name":"stdout","text":["< PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n","\n"]},{"output_type":"stream","name":"stderr","text":["[TRAIN]:   7%|▋         | 2/30 [05:59<1:11:19, 152.83s/it]\n","[EVAL]:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n","[EVAL]:   1%|          | 1/100 [00:00<00:10,  9.76it/s]\u001b[A\n","[EVAL]:   2%|▏         | 2/100 [00:00<00:10,  9.58it/s]\u001b[A\n","[EVAL]:   3%|▎         | 3/100 [00:00<00:10,  9.61it/s]\u001b[A\n","[EVAL]:   4%|▍         | 4/100 [00:00<00:09,  9.69it/s]\u001b[A\n","[EVAL]:   5%|▌         | 5/100 [00:00<00:09,  9.66it/s]\u001b[A\n","[EVAL]:   6%|▌         | 6/100 [00:00<00:09,  9.68it/s]\u001b[A\n","[EVAL]:   7%|▋         | 7/100 [00:00<00:09,  9.69it/s]\u001b[A\n","[EVAL]:   8%|▊         | 8/100 [00:00<00:09,  9.68it/s]\u001b[A\n","[EVAL]:   9%|▉         | 9/100 [00:00<00:09,  9.72it/s]\u001b[A\n","[EVAL]:  10%|█         | 10/100 [00:01<00:09,  9.77it/s]\u001b[A\n","[EVAL]:  11%|█         | 11/100 [00:01<00:09,  9.76it/s]\u001b[A\n","[EVAL]:  12%|█▏        | 12/100 [00:01<00:09,  9.76it/s]\u001b[A\n","[EVAL]:  13%|█▎        | 13/100 [00:01<00:08,  9.81it/s]\u001b[A\n","[EVAL]:  14%|█▍        | 14/100 [00:01<00:08,  9.70it/s]\u001b[A\n","[EVAL]:  15%|█▌        | 15/100 [00:01<00:08,  9.59it/s]\u001b[A\n","[EVAL]:  16%|█▌        | 16/100 [00:01<00:08,  9.35it/s]\u001b[A\n","[EVAL]:  17%|█▋        | 17/100 [00:01<00:08,  9.30it/s]\u001b[A\n","[EVAL]:  18%|█▊        | 18/100 [00:01<00:08,  9.31it/s]\u001b[A\n","[EVAL]:  19%|█▉        | 19/100 [00:01<00:08,  9.31it/s]\u001b[A\n","[EVAL]:  20%|██        | 20/100 [00:02<00:08,  9.25it/s]\u001b[A\n","[EVAL]:  21%|██        | 21/100 [00:02<00:08,  9.30it/s]\u001b[A\n","[EVAL]:  22%|██▏       | 22/100 [00:02<00:08,  9.25it/s]\u001b[A\n","[EVAL]:  23%|██▎       | 23/100 [00:02<00:08,  9.25it/s]\u001b[A\n","[EVAL]:  24%|██▍       | 24/100 [00:02<00:08,  9.20it/s]\u001b[A\n","[EVAL]:  25%|██▌       | 25/100 [00:02<00:08,  9.12it/s]\u001b[A\n","[EVAL]:  26%|██▌       | 26/100 [00:02<00:08,  9.13it/s]\u001b[A\n","[EVAL]:  27%|██▋       | 27/100 [00:02<00:07,  9.20it/s]\u001b[A\n","[EVAL]:  28%|██▊       | 28/100 [00:02<00:07,  9.28it/s]\u001b[A\n","[EVAL]:  29%|██▉       | 29/100 [00:03<00:07,  9.37it/s]\u001b[A\n","[EVAL]:  30%|███       | 30/100 [00:03<00:07,  9.40it/s]\u001b[A\n","[EVAL]:  31%|███       | 31/100 [00:03<00:07,  9.45it/s]\u001b[A\n","[EVAL]:  32%|███▏      | 32/100 [00:03<00:07,  9.49it/s]\u001b[A\n","[EVAL]:  33%|███▎      | 33/100 [00:03<00:07,  9.49it/s]\u001b[A\n","[EVAL]:  34%|███▍      | 34/100 [00:03<00:06,  9.53it/s]\u001b[A\n","[EVAL]:  35%|███▌      | 35/100 [00:03<00:06,  9.55it/s]\u001b[A\n","[EVAL]:  36%|███▌      | 36/100 [00:03<00:06,  9.56it/s]\u001b[A\n","[EVAL]:  37%|███▋      | 37/100 [00:03<00:06,  9.52it/s]\u001b[A\n","[EVAL]:  38%|███▊      | 38/100 [00:04<00:06,  9.51it/s]\u001b[A\n","[EVAL]:  39%|███▉      | 39/100 [00:04<00:06,  9.49it/s]\u001b[A\n","[EVAL]:  40%|████      | 40/100 [00:04<00:06,  9.59it/s]\u001b[A\n","[EVAL]:  41%|████      | 41/100 [00:04<00:06,  9.62it/s]\u001b[A\n","[EVAL]:  42%|████▏     | 42/100 [00:04<00:06,  9.66it/s]\u001b[A\n","[EVAL]:  43%|████▎     | 43/100 [00:04<00:05,  9.60it/s]\u001b[A\n","[EVAL]:  44%|████▍     | 44/100 [00:04<00:05,  9.53it/s]\u001b[A\n","[EVAL]:  45%|████▌     | 45/100 [00:04<00:05,  9.50it/s]\u001b[A\n","[EVAL]:  46%|████▌     | 46/100 [00:04<00:05,  9.53it/s]\u001b[A\n","[EVAL]:  47%|████▋     | 47/100 [00:04<00:05,  9.51it/s]\u001b[A\n","[EVAL]:  48%|████▊     | 48/100 [00:05<00:05,  9.47it/s]\u001b[A\n","[EVAL]:  49%|████▉     | 49/100 [00:05<00:05,  9.51it/s]\u001b[A\n","[EVAL]:  50%|█████     | 50/100 [00:05<00:05,  9.35it/s]\u001b[A\n","[EVAL]:  51%|█████     | 51/100 [00:05<00:05,  9.34it/s]\u001b[A\n","[EVAL]:  52%|█████▏    | 52/100 [00:05<00:05,  9.37it/s]\u001b[A\n","[EVAL]:  53%|█████▎    | 53/100 [00:05<00:04,  9.41it/s]\u001b[A\n","[EVAL]:  54%|█████▍    | 54/100 [00:05<00:04,  9.38it/s]\u001b[A\n","[EVAL]:  55%|█████▌    | 55/100 [00:05<00:04,  9.39it/s]\u001b[A\n","[EVAL]:  56%|█████▌    | 56/100 [00:05<00:04,  9.38it/s]\u001b[A\n","[EVAL]:  57%|█████▋    | 57/100 [00:06<00:04,  9.43it/s]\u001b[A\n","[EVAL]:  58%|█████▊    | 58/100 [00:06<00:04,  9.48it/s]\u001b[A\n","[EVAL]:  59%|█████▉    | 59/100 [00:06<00:04,  9.43it/s]\u001b[A\n","[EVAL]:  60%|██████    | 60/100 [00:06<00:04,  9.54it/s]\u001b[A\n","[EVAL]:  61%|██████    | 61/100 [00:06<00:04,  9.55it/s]\u001b[A\n","[EVAL]:  62%|██████▏   | 62/100 [00:06<00:04,  9.50it/s]\u001b[A\n","[EVAL]:  63%|██████▎   | 63/100 [00:06<00:03,  9.51it/s]\u001b[A\n","[EVAL]:  64%|██████▍   | 64/100 [00:06<00:03,  9.52it/s]\u001b[A\n","[EVAL]:  65%|██████▌   | 65/100 [00:06<00:03,  9.50it/s]\u001b[A\n","[EVAL]:  66%|██████▌   | 66/100 [00:06<00:03,  9.51it/s]\u001b[A\n","[EVAL]:  67%|██████▋   | 67/100 [00:07<00:03,  9.52it/s]\u001b[A\n","[EVAL]:  68%|██████▊   | 68/100 [00:07<00:03,  9.58it/s]\u001b[A\n","[EVAL]:  69%|██████▉   | 69/100 [00:07<00:03,  9.57it/s]\u001b[A\n","[EVAL]:  70%|███████   | 70/100 [00:07<00:03,  9.63it/s]\u001b[A\n","[EVAL]:  71%|███████   | 71/100 [00:07<00:03,  9.59it/s]\u001b[A\n","[EVAL]:  72%|███████▏  | 72/100 [00:07<00:02,  9.62it/s]\u001b[A\n","[EVAL]:  73%|███████▎  | 73/100 [00:07<00:02,  9.58it/s]\u001b[A\n","[EVAL]:  74%|███████▍  | 74/100 [00:07<00:02,  9.58it/s]\u001b[A\n","[EVAL]:  75%|███████▌  | 75/100 [00:07<00:02,  9.56it/s]\u001b[A\n","[EVAL]:  76%|███████▌  | 76/100 [00:08<00:02,  9.45it/s]\u001b[A\n","[EVAL]:  77%|███████▋  | 77/100 [00:08<00:02,  9.47it/s]\u001b[A\n","[EVAL]:  78%|███████▊  | 78/100 [00:08<00:02,  9.56it/s]\u001b[A\n","[EVAL]:  79%|███████▉  | 79/100 [00:08<00:02,  9.59it/s]\u001b[A\n","[EVAL]:  80%|████████  | 80/100 [00:08<00:02,  9.55it/s]\u001b[A\n","[EVAL]:  81%|████████  | 81/100 [00:08<00:01,  9.51it/s]\u001b[A\n","[EVAL]:  82%|████████▏ | 82/100 [00:08<00:01,  9.41it/s]\u001b[A\n","[EVAL]:  83%|████████▎ | 83/100 [00:08<00:01,  9.37it/s]\u001b[A\n","[EVAL]:  84%|████████▍ | 84/100 [00:08<00:01,  9.31it/s]\u001b[A\n","[EVAL]:  85%|████████▌ | 85/100 [00:08<00:01,  9.31it/s]\u001b[A\n","[EVAL]:  86%|████████▌ | 86/100 [00:09<00:01,  9.39it/s]\u001b[A\n","[EVAL]:  87%|████████▋ | 87/100 [00:09<00:01,  9.38it/s]\u001b[A\n","[EVAL]:  88%|████████▊ | 88/100 [00:09<00:01,  9.32it/s]\u001b[A\n","[EVAL]:  89%|████████▉ | 89/100 [00:09<00:01,  9.24it/s]\u001b[A\n","[EVAL]:  90%|█████████ | 90/100 [00:09<00:01,  9.20it/s]\u001b[A\n","[EVAL]:  91%|█████████ | 91/100 [00:09<00:00,  9.25it/s]\u001b[A\n","[EVAL]:  92%|█████████▏| 92/100 [00:09<00:00,  9.29it/s]\u001b[A\n","[EVAL]:  93%|█████████▎| 93/100 [00:09<00:00,  9.26it/s]\u001b[A\n","[EVAL]:  94%|█████████▍| 94/100 [00:09<00:00,  9.38it/s]\u001b[A\n","[EVAL]:  95%|█████████▌| 95/100 [00:10<00:00,  9.47it/s]\u001b[A\n","[EVAL]:  96%|█████████▌| 96/100 [00:10<00:00,  9.48it/s]\u001b[A\n","[EVAL]:  97%|█████████▋| 97/100 [00:10<00:00,  9.51it/s]\u001b[A\n","[EVAL]:  98%|█████████▊| 98/100 [00:10<00:00,  9.52it/s]\u001b[A\n","[EVAL]:  99%|█████████▉| 99/100 [00:10<00:00,  9.45it/s]\u001b[A\n","[EVAL]: 100%|██████████| 100/100 [00:10<00:00,  9.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch=2/30] Training Loss=0.7288. Test Loss = 3.6241. Test Perplexity = 49.50\n","> tu es tres intelligente\n","= you re very smart\n","< \n","\n","> je suis abasourdi\n","= i m flabbergasted\n","< \n","\n","> ca va EOS\n","= i m ok\n"]},{"output_type":"stream","name":"stderr","text":["\r[TRAIN]:  10%|█         | 3/30 [06:35<44:56, 99.87s/it]   "]},{"output_type":"stream","name":"stdout","text":["< \n","\n"]},{"output_type":"stream","name":"stderr","text":["[TRAIN]:  13%|█▎        | 4/30 [07:01<30:38, 70.69s/it]\n","[EVAL]:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n","[EVAL]:   1%|          | 1/100 [00:00<00:10,  9.66it/s]\u001b[A\n","[EVAL]:   2%|▏         | 2/100 [00:00<00:10,  9.69it/s]\u001b[A\n","[EVAL]:   3%|▎         | 3/100 [00:00<00:09,  9.70it/s]\u001b[A\n","[EVAL]:   4%|▍         | 4/100 [00:00<00:09,  9.64it/s]\u001b[A\n","[EVAL]:   5%|▌         | 5/100 [00:00<00:09,  9.59it/s]\u001b[A\n","[EVAL]:   6%|▌         | 6/100 [00:00<00:09,  9.51it/s]\u001b[A\n","[EVAL]:   7%|▋         | 7/100 [00:00<00:09,  9.44it/s]\u001b[A\n","[EVAL]:   8%|▊         | 8/100 [00:00<00:09,  9.36it/s]\u001b[A\n","[EVAL]:   9%|▉         | 9/100 [00:00<00:09,  9.40it/s]\u001b[A\n","[EVAL]:  10%|█         | 10/100 [00:01<00:09,  9.43it/s]\u001b[A\n","[EVAL]:  11%|█         | 11/100 [00:01<00:09,  9.42it/s]\u001b[A\n","[EVAL]:  12%|█▏        | 12/100 [00:01<00:09,  9.52it/s]\u001b[A\n","[EVAL]:  13%|█▎        | 13/100 [00:01<00:09,  9.49it/s]\u001b[A\n","[EVAL]:  14%|█▍        | 14/100 [00:01<00:09,  9.45it/s]\u001b[A\n","[EVAL]:  15%|█▌        | 15/100 [00:01<00:08,  9.47it/s]\u001b[A\n","[EVAL]:  16%|█▌        | 16/100 [00:01<00:08,  9.52it/s]\u001b[A\n","[EVAL]:  17%|█▋        | 17/100 [00:01<00:08,  9.43it/s]\u001b[A\n","[EVAL]:  18%|█▊        | 18/100 [00:01<00:08,  9.42it/s]\u001b[A\n","[EVAL]:  19%|█▉        | 19/100 [00:02<00:08,  9.39it/s]\u001b[A\n","[EVAL]:  20%|██        | 20/100 [00:02<00:08,  9.46it/s]\u001b[A\n","[EVAL]:  21%|██        | 21/100 [00:02<00:08,  9.46it/s]\u001b[A\n","[EVAL]:  22%|██▏       | 22/100 [00:02<00:08,  9.51it/s]\u001b[A\n","[EVAL]:  23%|██▎       | 23/100 [00:02<00:08,  9.50it/s]\u001b[A\n","[EVAL]:  24%|██▍       | 24/100 [00:02<00:08,  9.49it/s]\u001b[A\n","[EVAL]:  25%|██▌       | 25/100 [00:02<00:07,  9.46it/s]\u001b[A\n","[EVAL]:  26%|██▌       | 26/100 [00:02<00:07,  9.50it/s]\u001b[A\n","[EVAL]:  27%|██▋       | 27/100 [00:02<00:07,  9.57it/s]\u001b[A\n","[EVAL]:  28%|██▊       | 28/100 [00:02<00:07,  9.61it/s]\u001b[A\n","[EVAL]:  30%|███       | 30/100 [00:03<00:07,  9.78it/s]\u001b[A\n","[EVAL]:  31%|███       | 31/100 [00:03<00:07,  9.72it/s]\u001b[A\n","[EVAL]:  32%|███▏      | 32/100 [00:03<00:07,  9.71it/s]\u001b[A\n","[EVAL]:  33%|███▎      | 33/100 [00:03<00:06,  9.59it/s]\u001b[A\n","[EVAL]:  34%|███▍      | 34/100 [00:03<00:06,  9.58it/s]\u001b[A\n","[EVAL]:  35%|███▌      | 35/100 [00:03<00:06,  9.54it/s]\u001b[A\n","[EVAL]:  36%|███▌      | 36/100 [00:03<00:06,  9.54it/s]\u001b[A\n","[EVAL]:  37%|███▋      | 37/100 [00:03<00:06,  9.52it/s]\u001b[A\n","[EVAL]:  38%|███▊      | 38/100 [00:03<00:06,  9.58it/s]\u001b[A\n","[EVAL]:  39%|███▉      | 39/100 [00:04<00:06,  9.60it/s]\u001b[A\n","[EVAL]:  40%|████      | 40/100 [00:04<00:06,  9.56it/s]\u001b[A\n","[EVAL]:  41%|████      | 41/100 [00:04<00:06,  9.57it/s]\u001b[A\n","[EVAL]:  42%|████▏     | 42/100 [00:04<00:06,  9.42it/s]\u001b[A\n","[EVAL]:  43%|████▎     | 43/100 [00:04<00:06,  9.42it/s]\u001b[A\n","[EVAL]:  44%|████▍     | 44/100 [00:04<00:05,  9.52it/s]\u001b[A\n","[EVAL]:  45%|████▌     | 45/100 [00:04<00:05,  9.53it/s]\u001b[A\n","[EVAL]:  46%|████▌     | 46/100 [00:04<00:05,  9.57it/s]\u001b[A\n","[EVAL]:  47%|████▋     | 47/100 [00:04<00:05,  9.61it/s]\u001b[A\n","[EVAL]:  48%|████▊     | 48/100 [00:05<00:05,  9.66it/s]\u001b[A\n","[EVAL]:  49%|████▉     | 49/100 [00:05<00:05,  9.67it/s]\u001b[A\n","[EVAL]:  50%|█████     | 50/100 [00:05<00:05,  9.65it/s]\u001b[A\n","[EVAL]:  51%|█████     | 51/100 [00:05<00:05,  9.60it/s]\u001b[A\n","[EVAL]:  52%|█████▏    | 52/100 [00:05<00:05,  9.54it/s]\u001b[A\n","[EVAL]:  53%|█████▎    | 53/100 [00:05<00:04,  9.49it/s]\u001b[A\n","[EVAL]:  54%|█████▍    | 54/100 [00:05<00:04,  9.53it/s]\u001b[A\n","[EVAL]:  55%|█████▌    | 55/100 [00:05<00:04,  9.59it/s]\u001b[A\n","[EVAL]:  56%|█████▌    | 56/100 [00:05<00:04,  9.63it/s]\u001b[A\n","[EVAL]:  57%|█████▋    | 57/100 [00:05<00:04,  9.66it/s]\u001b[A\n","[EVAL]:  58%|█████▊    | 58/100 [00:06<00:04,  9.68it/s]\u001b[A\n","[EVAL]:  59%|█████▉    | 59/100 [00:06<00:04,  9.59it/s]\u001b[A\n","[EVAL]:  60%|██████    | 60/100 [00:06<00:04,  9.54it/s]\u001b[A\n","[EVAL]:  61%|██████    | 61/100 [00:06<00:04,  9.54it/s]\u001b[A\n","[EVAL]:  62%|██████▏   | 62/100 [00:06<00:04,  9.49it/s]\u001b[A\n","[EVAL]:  63%|██████▎   | 63/100 [00:06<00:03,  9.49it/s]\u001b[A\n","[EVAL]:  64%|██████▍   | 64/100 [00:06<00:03,  9.50it/s]\u001b[A\n","[EVAL]:  65%|██████▌   | 65/100 [00:06<00:03,  9.31it/s]\u001b[A\n","[EVAL]:  66%|██████▌   | 66/100 [00:06<00:03,  9.18it/s]\u001b[A\n","[EVAL]:  67%|██████▋   | 67/100 [00:07<00:03,  9.08it/s]\u001b[A\n","[EVAL]:  68%|██████▊   | 68/100 [00:07<00:03,  9.23it/s]\u001b[A\n","[EVAL]:  69%|██████▉   | 69/100 [00:07<00:03,  9.20it/s]\u001b[A\n","[EVAL]:  70%|███████   | 70/100 [00:07<00:03,  9.08it/s]\u001b[A\n","[EVAL]:  71%|███████   | 71/100 [00:07<00:03,  9.20it/s]\u001b[A\n","[EVAL]:  72%|███████▏  | 72/100 [00:07<00:03,  9.15it/s]\u001b[A\n","[EVAL]:  73%|███████▎  | 73/100 [00:07<00:02,  9.24it/s]\u001b[A\n","[EVAL]:  74%|███████▍  | 74/100 [00:07<00:02,  9.07it/s]\u001b[A\n","[EVAL]:  75%|███████▌  | 75/100 [00:07<00:02,  9.09it/s]\u001b[A\n","[EVAL]:  76%|███████▌  | 76/100 [00:08<00:02,  9.17it/s]\u001b[A\n","[EVAL]:  77%|███████▋  | 77/100 [00:08<00:02,  9.22it/s]\u001b[A\n","[EVAL]:  78%|███████▊  | 78/100 [00:08<00:02,  9.35it/s]\u001b[A\n","[EVAL]:  79%|███████▉  | 79/100 [00:08<00:02,  9.37it/s]\u001b[A\n","[EVAL]:  80%|████████  | 80/100 [00:08<00:02,  9.41it/s]\u001b[A\n","[EVAL]:  81%|████████  | 81/100 [00:08<00:02,  9.39it/s]\u001b[A\n","[EVAL]:  82%|████████▏ | 82/100 [00:08<00:01,  9.25it/s]\u001b[A\n","[EVAL]:  83%|████████▎ | 83/100 [00:08<00:01,  9.19it/s]\u001b[A\n","[EVAL]:  84%|████████▍ | 84/100 [00:08<00:01,  9.21it/s]\u001b[A\n","[EVAL]:  85%|████████▌ | 85/100 [00:08<00:01,  9.18it/s]\u001b[A\n","[EVAL]:  86%|████████▌ | 86/100 [00:09<00:01,  9.27it/s]\u001b[A\n","[EVAL]:  87%|████████▋ | 87/100 [00:09<00:01,  9.34it/s]\u001b[A\n","[EVAL]:  88%|████████▊ | 88/100 [00:09<00:01,  9.36it/s]\u001b[A\n","[EVAL]:  89%|████████▉ | 89/100 [00:09<00:01,  9.46it/s]\u001b[A\n","[EVAL]:  90%|█████████ | 90/100 [00:09<00:01,  9.45it/s]\u001b[A\n","[EVAL]:  91%|█████████ | 91/100 [00:09<00:00,  9.50it/s]\u001b[A\n","[EVAL]:  92%|█████████▏| 92/100 [00:09<00:00,  9.47it/s]\u001b[A\n","[EVAL]:  93%|█████████▎| 93/100 [00:09<00:00,  9.46it/s]\u001b[A\n","[EVAL]:  94%|█████████▍| 94/100 [00:09<00:00,  9.44it/s]\u001b[A\n","[EVAL]:  95%|█████████▌| 95/100 [00:10<00:00,  9.43it/s]\u001b[A\n","[EVAL]:  96%|█████████▌| 96/100 [00:10<00:00,  9.46it/s]\u001b[A\n","[EVAL]:  97%|█████████▋| 97/100 [00:10<00:00,  9.49it/s]\u001b[A\n","[EVAL]:  98%|█████████▊| 98/100 [00:10<00:00,  9.59it/s]\u001b[A\n","[EVAL]:  99%|█████████▉| 99/100 [00:10<00:00,  9.60it/s]\u001b[A\n","[EVAL]: 100%|██████████| 100/100 [00:10<00:00,  9.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch=4/30] Training Loss=0.6788. Test Loss = 3.3544. Test Perplexity = 36.44\n","> elles sont toutes mortes\n","= they re all dead\n","< \n","\n","> ca va EOS\n","= i m ok\n","< \n","\n","> vous etes avec des\n","= you re with friends\n"]},{"output_type":"stream","name":"stderr","text":["\r[TRAIN]:  17%|█▋        | 5/30 [07:38<24:21, 58.45s/it]"]},{"output_type":"stream","name":"stdout","text":["< \n","\n"]},{"output_type":"stream","name":"stderr","text":["[TRAIN]:  20%|██        | 6/30 [08:03<18:51, 47.13s/it]\n","[EVAL]:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n","[EVAL]:   1%|          | 1/100 [00:00<00:10,  9.33it/s]\u001b[A\n","[EVAL]:   2%|▏         | 2/100 [00:00<00:10,  9.37it/s]\u001b[A\n","[EVAL]:   3%|▎         | 3/100 [00:00<00:10,  9.35it/s]\u001b[A\n","[EVAL]:   4%|▍         | 4/100 [00:00<00:10,  9.35it/s]\u001b[A\n","[EVAL]:   5%|▌         | 5/100 [00:00<00:10,  9.37it/s]\u001b[A\n","[EVAL]:   6%|▌         | 6/100 [00:00<00:10,  9.37it/s]\u001b[A\n","[EVAL]:   7%|▋         | 7/100 [00:00<00:09,  9.40it/s]\u001b[A\n","[EVAL]:   8%|▊         | 8/100 [00:00<00:09,  9.45it/s]\u001b[A\n","[EVAL]:   9%|▉         | 9/100 [00:00<00:09,  9.39it/s]\u001b[A\n","[EVAL]:  10%|█         | 10/100 [00:01<00:09,  9.34it/s]\u001b[A\n","[EVAL]:  11%|█         | 11/100 [00:01<00:09,  9.30it/s]\u001b[A\n","[EVAL]:  12%|█▏        | 12/100 [00:01<00:09,  9.40it/s]\u001b[A\n","[EVAL]:  13%|█▎        | 13/100 [00:01<00:09,  9.45it/s]\u001b[A\n","[EVAL]:  14%|█▍        | 14/100 [00:01<00:09,  9.38it/s]\u001b[A\n","[EVAL]:  15%|█▌        | 15/100 [00:01<00:09,  9.35it/s]\u001b[A\n","[EVAL]:  16%|█▌        | 16/100 [00:01<00:08,  9.39it/s]\u001b[A\n","[EVAL]:  17%|█▋        | 17/100 [00:01<00:08,  9.45it/s]\u001b[A\n","[EVAL]:  18%|█▊        | 18/100 [00:01<00:08,  9.46it/s]\u001b[A\n","[EVAL]:  19%|█▉        | 19/100 [00:02<00:08,  9.46it/s]\u001b[A\n","[EVAL]:  20%|██        | 20/100 [00:02<00:08,  9.45it/s]\u001b[A\n","[EVAL]:  21%|██        | 21/100 [00:02<00:08,  9.15it/s]\u001b[A\n","[EVAL]:  22%|██▏       | 22/100 [00:02<00:08,  9.22it/s]\u001b[A\n","[EVAL]:  23%|██▎       | 23/100 [00:02<00:08,  9.31it/s]\u001b[A\n","[EVAL]:  24%|██▍       | 24/100 [00:02<00:08,  9.30it/s]\u001b[A\n","[EVAL]:  25%|██▌       | 25/100 [00:02<00:08,  9.27it/s]\u001b[A\n","[EVAL]:  26%|██▌       | 26/100 [00:02<00:07,  9.35it/s]\u001b[A\n","[EVAL]:  27%|██▋       | 27/100 [00:02<00:07,  9.46it/s]\u001b[A\n","[EVAL]:  28%|██▊       | 28/100 [00:02<00:07,  9.28it/s]\u001b[A\n","[EVAL]:  29%|██▉       | 29/100 [00:03<00:07,  9.29it/s]\u001b[A\n","[EVAL]:  30%|███       | 30/100 [00:03<00:07,  9.24it/s]\u001b[A\n","[EVAL]:  31%|███       | 31/100 [00:03<00:07,  9.10it/s]\u001b[A\n","[EVAL]:  32%|███▏      | 32/100 [00:03<00:07,  9.07it/s]\u001b[A\n","[EVAL]:  33%|███▎      | 33/100 [00:03<00:07,  9.15it/s]\u001b[A\n","[EVAL]:  34%|███▍      | 34/100 [00:03<00:07,  9.17it/s]\u001b[A\n","[EVAL]:  35%|███▌      | 35/100 [00:03<00:07,  9.25it/s]\u001b[A\n","[EVAL]:  36%|███▌      | 36/100 [00:03<00:06,  9.33it/s]\u001b[A\n","[EVAL]:  37%|███▋      | 37/100 [00:03<00:06,  9.45it/s]\u001b[A\n","[EVAL]:  38%|███▊      | 38/100 [00:04<00:06,  9.55it/s]\u001b[A\n","[EVAL]:  39%|███▉      | 39/100 [00:04<00:06,  9.60it/s]\u001b[A\n","[EVAL]:  40%|████      | 40/100 [00:04<00:06,  9.63it/s]\u001b[A\n","[EVAL]:  41%|████      | 41/100 [00:04<00:06,  9.57it/s]\u001b[A\n","[EVAL]:  42%|████▏     | 42/100 [00:04<00:06,  9.52it/s]\u001b[A\n","[EVAL]:  43%|████▎     | 43/100 [00:04<00:06,  9.43it/s]\u001b[A\n","[EVAL]:  44%|████▍     | 44/100 [00:04<00:05,  9.40it/s]\u001b[A\n","[EVAL]:  45%|████▌     | 45/100 [00:04<00:05,  9.42it/s]\u001b[A\n","[EVAL]:  46%|████▌     | 46/100 [00:04<00:05,  9.43it/s]\u001b[A\n","[EVAL]:  47%|████▋     | 47/100 [00:05<00:05,  9.49it/s]\u001b[A\n","[EVAL]:  48%|████▊     | 48/100 [00:05<00:05,  9.50it/s]\u001b[A\n","[EVAL]:  49%|████▉     | 49/100 [00:05<00:05,  9.44it/s]\u001b[A\n","[EVAL]:  50%|█████     | 50/100 [00:05<00:05,  9.46it/s]\u001b[A\n","[EVAL]:  51%|█████     | 51/100 [00:05<00:05,  9.39it/s]\u001b[A\n","[EVAL]:  52%|█████▏    | 52/100 [00:05<00:05,  9.42it/s]\u001b[A\n","[EVAL]:  53%|█████▎    | 53/100 [00:05<00:05,  9.40it/s]\u001b[A\n","[EVAL]:  54%|█████▍    | 54/100 [00:05<00:04,  9.42it/s]\u001b[A\n","[EVAL]:  55%|█████▌    | 55/100 [00:05<00:04,  9.48it/s]\u001b[A\n","[EVAL]:  56%|█████▌    | 56/100 [00:05<00:04,  9.48it/s]\u001b[A\n","[EVAL]:  57%|█████▋    | 57/100 [00:06<00:04,  9.47it/s]\u001b[A\n","[EVAL]:  58%|█████▊    | 58/100 [00:06<00:04,  9.48it/s]\u001b[A\n","[EVAL]:  59%|█████▉    | 59/100 [00:06<00:04,  9.32it/s]\u001b[A\n","[EVAL]:  60%|██████    | 60/100 [00:06<00:04,  9.10it/s]\u001b[A\n","[EVAL]:  61%|██████    | 61/100 [00:06<00:04,  9.07it/s]\u001b[A\n","[EVAL]:  62%|██████▏   | 62/100 [00:06<00:04,  9.12it/s]\u001b[A\n","[EVAL]:  63%|██████▎   | 63/100 [00:06<00:04,  9.13it/s]\u001b[A\n","[EVAL]:  64%|██████▍   | 64/100 [00:06<00:03,  9.28it/s]\u001b[A\n","[EVAL]:  65%|██████▌   | 65/100 [00:06<00:03,  9.34it/s]\u001b[A\n","[EVAL]:  66%|██████▌   | 66/100 [00:07<00:03,  9.34it/s]\u001b[A\n","[EVAL]:  67%|██████▋   | 67/100 [00:07<00:03,  9.42it/s]\u001b[A\n","[EVAL]:  68%|██████▊   | 68/100 [00:07<00:03,  9.42it/s]\u001b[A\n","[EVAL]:  69%|██████▉   | 69/100 [00:07<00:03,  9.49it/s]\u001b[A\n","[EVAL]:  70%|███████   | 70/100 [00:07<00:03,  9.57it/s]\u001b[A\n","[EVAL]:  71%|███████   | 71/100 [00:07<00:03,  9.56it/s]\u001b[A\n","[EVAL]:  72%|███████▏  | 72/100 [00:07<00:02,  9.50it/s]\u001b[A\n","[EVAL]:  73%|███████▎  | 73/100 [00:07<00:02,  9.48it/s]\u001b[A\n","[EVAL]:  74%|███████▍  | 74/100 [00:07<00:02,  9.59it/s]\u001b[A\n","[EVAL]:  75%|███████▌  | 75/100 [00:07<00:02,  9.59it/s]\u001b[A\n","[EVAL]:  76%|███████▌  | 76/100 [00:08<00:02,  9.53it/s]\u001b[A\n","[EVAL]:  77%|███████▋  | 77/100 [00:08<00:02,  9.45it/s]\u001b[A\n","[EVAL]:  78%|███████▊  | 78/100 [00:08<00:02,  9.43it/s]\u001b[A\n","[EVAL]:  79%|███████▉  | 79/100 [00:08<00:02,  9.41it/s]\u001b[A\n","[EVAL]:  80%|████████  | 80/100 [00:08<00:02,  9.42it/s]\u001b[A\n","[EVAL]:  81%|████████  | 81/100 [00:08<00:02,  9.38it/s]\u001b[A\n","[EVAL]:  82%|████████▏ | 82/100 [00:08<00:01,  9.46it/s]\u001b[A\n","[EVAL]:  83%|████████▎ | 83/100 [00:08<00:01,  9.52it/s]\u001b[A\n","[EVAL]:  84%|████████▍ | 84/100 [00:08<00:01,  9.55it/s]\u001b[A\n","[EVAL]:  85%|████████▌ | 85/100 [00:09<00:01,  9.59it/s]\u001b[A\n","[EVAL]:  86%|████████▌ | 86/100 [00:09<00:01,  9.54it/s]\u001b[A\n","[EVAL]:  87%|████████▋ | 87/100 [00:09<00:01,  9.52it/s]\u001b[A\n","[EVAL]:  88%|████████▊ | 88/100 [00:09<00:01,  9.42it/s]\u001b[A\n","[EVAL]:  89%|████████▉ | 89/100 [00:09<00:01,  9.36it/s]\u001b[A\n","[EVAL]:  90%|█████████ | 90/100 [00:09<00:01,  9.36it/s]\u001b[A\n","[EVAL]:  91%|█████████ | 91/100 [00:09<00:00,  9.37it/s]\u001b[A\n","[EVAL]:  92%|█████████▏| 92/100 [00:09<00:00,  9.40it/s]\u001b[A\n","[EVAL]:  93%|█████████▎| 93/100 [00:09<00:00,  9.40it/s]\u001b[A\n","[EVAL]:  94%|█████████▍| 94/100 [00:10<00:00,  9.36it/s]\u001b[A\n","[EVAL]:  95%|█████████▌| 95/100 [00:10<00:00,  9.48it/s]\u001b[A\n","[EVAL]:  96%|█████████▌| 96/100 [00:10<00:00,  9.46it/s]\u001b[A\n","[EVAL]:  97%|█████████▋| 97/100 [00:10<00:00,  9.53it/s]\u001b[A\n","[EVAL]:  98%|█████████▊| 98/100 [00:10<00:00,  9.55it/s]\u001b[A\n","[EVAL]:  99%|█████████▉| 99/100 [00:10<00:00,  9.57it/s]\u001b[A\n","[EVAL]: 100%|██████████| 100/100 [00:10<00:00,  9.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch=6/30] Training Loss=0.9617. Test Loss = 3.5151. Test Perplexity = 43.50\n","> tu es bougon\n","= you re grumpy\n","< \n","\n","> je suis abasourdi\n","= i m flabbergasted\n","< \n","\n","> nous sommes trop en\n","= we re too late\n"]},{"output_type":"stream","name":"stderr","text":["\r[TRAIN]:  23%|██▎       | 7/30 [08:39<16:40, 43.50s/it]"]},{"output_type":"stream","name":"stdout","text":["< \n","\n"]},{"output_type":"stream","name":"stderr","text":["[TRAIN]:  27%|██▋       | 8/30 [09:04<13:46, 37.56s/it]\n","[EVAL]:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n","[EVAL]:   1%|          | 1/100 [00:02<04:34,  2.77s/it]\u001b[A\n","[EVAL]:   2%|▏         | 2/100 [00:05<04:32,  2.78s/it]\u001b[A\n","[EVAL]:   3%|▎         | 3/100 [00:08<04:29,  2.78s/it]\u001b[A\n","[EVAL]:   4%|▍         | 4/100 [00:11<04:31,  2.83s/it]\u001b[A\n","[EVAL]:   5%|▌         | 5/100 [00:14<04:27,  2.82s/it]\u001b[A\n","[EVAL]:   6%|▌         | 6/100 [00:16<04:23,  2.81s/it]\u001b[A\n","[EVAL]:   7%|▋         | 7/100 [00:19<04:20,  2.80s/it]\u001b[A\n","[EVAL]:   8%|▊         | 8/100 [00:22<04:18,  2.81s/it]\u001b[A\n","[EVAL]:   9%|▉         | 9/100 [00:25<04:15,  2.81s/it]\u001b[A\n","[EVAL]:  10%|█         | 10/100 [00:27<04:11,  2.79s/it]\u001b[A\n","[EVAL]:  11%|█         | 11/100 [00:30<04:07,  2.78s/it]\u001b[A\n","[EVAL]:  12%|█▏        | 12/100 [00:33<04:05,  2.79s/it]\u001b[A\n","[EVAL]:  13%|█▎        | 13/100 [00:36<04:01,  2.78s/it]\u001b[A\n","[EVAL]:  14%|█▍        | 14/100 [00:39<03:59,  2.78s/it]\u001b[A\n","[EVAL]:  15%|█▌        | 15/100 [00:41<03:56,  2.79s/it]\u001b[A\n","[EVAL]:  16%|█▌        | 16/100 [00:44<03:55,  2.81s/it]\u001b[A\n","[EVAL]:  17%|█▋        | 17/100 [00:47<03:52,  2.80s/it]\u001b[A\n","[EVAL]:  18%|█▊        | 18/100 [00:50<03:49,  2.80s/it]\u001b[A\n","[EVAL]:  19%|█▉        | 19/100 [00:53<03:46,  2.79s/it]\u001b[A\n","[EVAL]:  20%|██        | 20/100 [00:56<03:45,  2.82s/it]\u001b[A\n","[EVAL]:  21%|██        | 21/100 [00:58<03:41,  2.81s/it]\u001b[A\n","[EVAL]:  22%|██▏       | 22/100 [01:01<03:38,  2.80s/it]\u001b[A\n","[EVAL]:  23%|██▎       | 23/100 [01:04<03:34,  2.78s/it]\u001b[A\n","[EVAL]:  24%|██▍       | 24/100 [01:07<03:32,  2.79s/it]\u001b[A\n","[EVAL]:  25%|██▌       | 25/100 [01:09<03:29,  2.80s/it]\u001b[A\n","[EVAL]:  26%|██▌       | 26/100 [01:12<03:27,  2.81s/it]\u001b[A\n","[EVAL]:  27%|██▋       | 27/100 [01:15<03:24,  2.80s/it]\u001b[A\n","[EVAL]:  28%|██▊       | 28/100 [01:18<03:22,  2.81s/it]\u001b[A\n","[EVAL]:  29%|██▉       | 29/100 [01:21<03:18,  2.80s/it]\u001b[A\n","[EVAL]:  30%|███       | 30/100 [01:23<03:14,  2.78s/it]\u001b[A\n","[EVAL]:  31%|███       | 31/100 [01:26<03:13,  2.81s/it]\u001b[A\n","[EVAL]:  32%|███▏      | 32/100 [01:29<03:11,  2.81s/it]\u001b[A\n","[EVAL]:  33%|███▎      | 33/100 [01:32<03:08,  2.81s/it]\u001b[A\n","[EVAL]:  34%|███▍      | 34/100 [01:35<03:05,  2.81s/it]\u001b[A\n","[EVAL]:  35%|███▌      | 35/100 [01:37<03:02,  2.80s/it]\u001b[A\n","[EVAL]:  36%|███▌      | 36/100 [01:40<03:00,  2.82s/it]\u001b[A\n","[EVAL]:  37%|███▋      | 37/100 [01:43<02:58,  2.83s/it]\u001b[A\n","[EVAL]:  38%|███▊      | 38/100 [01:46<02:55,  2.84s/it]\u001b[A\n","[EVAL]:  39%|███▉      | 39/100 [01:49<02:52,  2.83s/it]\u001b[A\n","[EVAL]:  40%|████      | 40/100 [01:52<02:50,  2.85s/it]\u001b[A\n","[EVAL]:  41%|████      | 41/100 [01:55<02:47,  2.84s/it]\u001b[A\n","[EVAL]:  42%|████▏     | 42/100 [01:57<02:45,  2.85s/it]\u001b[A\n","[EVAL]:  43%|████▎     | 43/100 [02:00<02:41,  2.83s/it]\u001b[A\n","[EVAL]:  44%|████▍     | 44/100 [02:03<02:38,  2.82s/it]\u001b[A\n","[EVAL]:  45%|████▌     | 45/100 [02:06<02:34,  2.80s/it]\u001b[A\n","[EVAL]:  46%|████▌     | 46/100 [02:09<02:30,  2.79s/it]\u001b[A\n","[EVAL]:  47%|████▋     | 47/100 [02:11<02:27,  2.79s/it]\u001b[A\n","[EVAL]:  48%|████▊     | 48/100 [02:14<02:25,  2.80s/it]\u001b[A\n","[EVAL]:  49%|████▉     | 49/100 [02:17<02:21,  2.78s/it]\u001b[A\n","[EVAL]:  50%|█████     | 50/100 [02:20<02:18,  2.78s/it]\u001b[A\n","[EVAL]:  51%|█████     | 51/100 [02:22<02:16,  2.78s/it]\u001b[A\n","[EVAL]:  52%|█████▏    | 52/100 [02:25<02:13,  2.79s/it]\u001b[A\n","[EVAL]:  53%|█████▎    | 53/100 [02:28<02:11,  2.80s/it]\u001b[A\n","[EVAL]:  54%|█████▍    | 54/100 [02:31<02:08,  2.79s/it]\u001b[A\n","[EVAL]:  55%|█████▌    | 55/100 [02:34<02:05,  2.78s/it]\u001b[A\n","[EVAL]:  56%|█████▌    | 56/100 [02:36<02:02,  2.78s/it]\u001b[A\n","[EVAL]:  57%|█████▋    | 57/100 [02:39<01:59,  2.78s/it]\u001b[A\n","[EVAL]:  58%|█████▊    | 58/100 [02:42<01:56,  2.78s/it]\u001b[A\n","[EVAL]:  59%|█████▉    | 59/100 [02:45<01:53,  2.77s/it]\u001b[A\n","[EVAL]:  60%|██████    | 60/100 [02:47<01:50,  2.77s/it]\u001b[A\n","[EVAL]:  61%|██████    | 61/100 [02:50<01:48,  2.78s/it]\u001b[A\n","[EVAL]:  62%|██████▏   | 62/100 [02:53<01:46,  2.81s/it]\u001b[A\n","[EVAL]:  63%|██████▎   | 63/100 [02:56<01:43,  2.81s/it]\u001b[A\n","[EVAL]:  64%|██████▍   | 64/100 [02:59<01:41,  2.82s/it]\u001b[A\n","[EVAL]:  65%|██████▌   | 65/100 [03:02<01:38,  2.82s/it]\u001b[A\n","[EVAL]:  66%|██████▌   | 66/100 [03:04<01:35,  2.81s/it]\u001b[A\n","[EVAL]:  67%|██████▋   | 67/100 [03:07<01:31,  2.79s/it]\u001b[A\n","[EVAL]:  68%|██████▊   | 68/100 [03:10<01:28,  2.78s/it]\u001b[A\n","[EVAL]:  69%|██████▉   | 69/100 [03:13<01:26,  2.78s/it]\u001b[A\n","[EVAL]:  70%|███████   | 70/100 [03:15<01:23,  2.78s/it]\u001b[A\n","[EVAL]:  71%|███████   | 71/100 [03:18<01:20,  2.77s/it]\u001b[A\n","[EVAL]:  72%|███████▏  | 72/100 [03:21<01:17,  2.77s/it]\u001b[A\n","[EVAL]:  73%|███████▎  | 73/100 [03:24<01:14,  2.77s/it]\u001b[A\n","[EVAL]:  74%|███████▍  | 74/100 [03:27<01:12,  2.78s/it]\u001b[A\n","[EVAL]:  75%|███████▌  | 75/100 [03:29<01:09,  2.79s/it]\u001b[A\n","[EVAL]:  76%|███████▌  | 76/100 [03:32<01:06,  2.79s/it]\u001b[A\n","[EVAL]:  77%|███████▋  | 77/100 [03:35<01:04,  2.80s/it]\u001b[A\n","[EVAL]:  78%|███████▊  | 78/100 [03:38<01:01,  2.78s/it]\u001b[A\n","[EVAL]:  79%|███████▉  | 79/100 [03:40<00:58,  2.78s/it]\u001b[A\n","[EVAL]:  80%|████████  | 80/100 [03:43<00:55,  2.78s/it]\u001b[A\n","[EVAL]:  81%|████████  | 81/100 [03:46<00:53,  2.81s/it]\u001b[A\n","[EVAL]:  82%|████████▏ | 82/100 [03:49<00:50,  2.79s/it]\u001b[A\n","[EVAL]:  83%|████████▎ | 83/100 [03:52<00:47,  2.78s/it]\u001b[A\n","[EVAL]:  84%|████████▍ | 84/100 [03:54<00:44,  2.78s/it]\u001b[A\n","[EVAL]:  85%|████████▌ | 85/100 [03:57<00:41,  2.80s/it]\u001b[A\n","[EVAL]:  86%|████████▌ | 86/100 [04:00<00:38,  2.78s/it]\u001b[A\n","[EVAL]:  87%|████████▋ | 87/100 [04:03<00:35,  2.76s/it]\u001b[A\n","[EVAL]:  88%|████████▊ | 88/100 [04:05<00:33,  2.76s/it]\u001b[A\n","[EVAL]:  89%|████████▉ | 89/100 [04:08<00:30,  2.79s/it]\u001b[A\n","[EVAL]:  90%|█████████ | 90/100 [04:11<00:27,  2.79s/it]\u001b[A\n","[EVAL]:  91%|█████████ | 91/100 [04:14<00:25,  2.80s/it]\u001b[A\n","[EVAL]:  92%|█████████▏| 92/100 [04:17<00:22,  2.77s/it]\u001b[A\n","[EVAL]:  93%|█████████▎| 93/100 [04:19<00:19,  2.77s/it]\u001b[A\n","[EVAL]:  94%|█████████▍| 94/100 [04:22<00:16,  2.77s/it]\u001b[A\n","[EVAL]:  95%|█████████▌| 95/100 [04:25<00:13,  2.77s/it]\u001b[A\n","[EVAL]:  96%|█████████▌| 96/100 [04:28<00:11,  2.78s/it]\u001b[A\n","[EVAL]:  97%|█████████▋| 97/100 [04:31<00:08,  2.78s/it]\u001b[A\n","[EVAL]:  98%|█████████▊| 98/100 [04:33<00:05,  2.78s/it]\u001b[A\n","[EVAL]:  99%|█████████▉| 99/100 [04:36<00:02,  2.78s/it]\u001b[A\n","[EVAL]: 100%|██████████| 100/100 [04:39<00:00,  2.79s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch=8/30] Training Loss=1.0378. Test Loss = 0.6993. Test Perplexity = 2.02\n","> nous y sommes\n","= we are here\n","< PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n","\n","> je suis abasourdi\n","= i m flabbergasted\n","< PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n","\n","> vous etes jeune\n","= you re young\n"]},{"output_type":"stream","name":"stderr","text":["\r[TRAIN]:  30%|███       | 9/30 [14:17<43:16, 123.66s/it]"]},{"output_type":"stream","name":"stdout","text":["< PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n","\n"]},{"output_type":"stream","name":"stderr","text":["[TRAIN]:  33%|███▎      | 10/30 [14:42<31:05, 93.27s/it]\n","[EVAL]:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n","[EVAL]:   1%|          | 1/100 [00:02<04:32,  2.76s/it]\u001b[A\n","[EVAL]:   2%|▏         | 2/100 [00:05<04:30,  2.76s/it]\u001b[A\n","[EVAL]:   3%|▎         | 3/100 [00:08<04:28,  2.77s/it]\u001b[A\n","[EVAL]:   4%|▍         | 4/100 [00:11<04:24,  2.75s/it]\u001b[A\n","[EVAL]:   5%|▌         | 5/100 [00:13<04:21,  2.76s/it]\u001b[A\n","[EVAL]:   6%|▌         | 6/100 [00:16<04:17,  2.74s/it]\u001b[A\n","[EVAL]:   7%|▋         | 7/100 [00:19<04:14,  2.74s/it]\u001b[A\n","[EVAL]:   8%|▊         | 8/100 [00:22<04:13,  2.75s/it]\u001b[A\n","[EVAL]:   9%|▉         | 9/100 [00:24<04:12,  2.77s/it]\u001b[A\n","[EVAL]:  10%|█         | 10/100 [00:27<04:07,  2.75s/it]\u001b[A\n","[EVAL]:  11%|█         | 11/100 [00:30<04:04,  2.75s/it]\u001b[A\n","[EVAL]:  12%|█▏        | 12/100 [00:33<04:02,  2.75s/it]\u001b[A\n","[EVAL]:  13%|█▎        | 13/100 [00:35<04:00,  2.77s/it]\u001b[A\n","[EVAL]:  14%|█▍        | 14/100 [00:38<03:56,  2.75s/it]\u001b[A\n","[EVAL]:  15%|█▌        | 15/100 [00:41<03:53,  2.75s/it]\u001b[A\n","[EVAL]:  16%|█▌        | 16/100 [00:44<03:50,  2.75s/it]\u001b[A\n","[EVAL]:  17%|█▋        | 17/100 [00:46<03:49,  2.76s/it]\u001b[A\n","[EVAL]:  18%|█▊        | 18/100 [00:49<03:45,  2.74s/it]\u001b[A\n","[EVAL]:  19%|█▉        | 19/100 [00:52<03:44,  2.77s/it]\u001b[A\n","[EVAL]:  20%|██        | 20/100 [00:55<03:40,  2.76s/it]\u001b[A\n","[EVAL]:  21%|██        | 21/100 [00:57<03:37,  2.75s/it]\u001b[A\n","[EVAL]:  22%|██▏       | 22/100 [01:00<03:34,  2.75s/it]\u001b[A\n","[EVAL]:  23%|██▎       | 23/100 [01:03<03:31,  2.75s/it]\u001b[A\n","[EVAL]:  24%|██▍       | 24/100 [01:06<03:31,  2.78s/it]\u001b[A\n","[EVAL]:  25%|██▌       | 25/100 [01:09<03:29,  2.79s/it]\u001b[A\n","[EVAL]:  26%|██▌       | 26/100 [01:11<03:26,  2.79s/it]\u001b[A\n","[EVAL]:  27%|██▋       | 27/100 [01:14<03:23,  2.79s/it]\u001b[A\n","[EVAL]:  28%|██▊       | 28/100 [01:17<03:20,  2.78s/it]\u001b[A\n","[EVAL]:  29%|██▉       | 29/100 [01:20<03:20,  2.83s/it]\u001b[A\n","[EVAL]:  30%|███       | 30/100 [01:23<03:18,  2.83s/it]\u001b[A\n","[EVAL]:  31%|███       | 31/100 [01:25<03:13,  2.81s/it]\u001b[A\n","[EVAL]:  32%|███▏      | 32/100 [01:28<03:10,  2.80s/it]\u001b[A\n","[EVAL]:  33%|███▎      | 33/100 [01:31<03:06,  2.78s/it]\u001b[A\n","[EVAL]:  34%|███▍      | 34/100 [01:34<03:02,  2.77s/it]\u001b[A\n","[EVAL]:  35%|███▌      | 35/100 [01:36<02:59,  2.76s/it]\u001b[A\n","[EVAL]:  36%|███▌      | 36/100 [01:39<02:56,  2.76s/it]\u001b[A\n","[EVAL]:  37%|███▋      | 37/100 [01:42<02:54,  2.77s/it]\u001b[A\n","[EVAL]:  38%|███▊      | 38/100 [01:45<02:50,  2.76s/it]\u001b[A\n","[EVAL]:  39%|███▉      | 39/100 [01:47<02:47,  2.75s/it]\u001b[A\n","[EVAL]:  40%|████      | 40/100 [01:50<02:44,  2.75s/it]\u001b[A\n","[EVAL]:  41%|████      | 41/100 [01:53<02:43,  2.76s/it]\u001b[A\n","[EVAL]:  42%|████▏     | 42/100 [01:56<02:40,  2.76s/it]\u001b[A\n","[EVAL]:  43%|████▎     | 43/100 [01:58<02:36,  2.75s/it]\u001b[A\n","[EVAL]:  44%|████▍     | 44/100 [02:01<02:33,  2.74s/it]\u001b[A\n","[EVAL]:  45%|████▌     | 45/100 [02:04<02:30,  2.73s/it]\u001b[A\n","[EVAL]:  46%|████▌     | 46/100 [02:07<02:28,  2.75s/it]\u001b[A\n","[EVAL]:  47%|████▋     | 47/100 [02:09<02:25,  2.74s/it]\u001b[A\n","[EVAL]:  48%|████▊     | 48/100 [02:12<02:22,  2.73s/it]\u001b[A\n","[EVAL]:  49%|████▉     | 49/100 [02:15<02:20,  2.75s/it]\u001b[A\n","[EVAL]:  50%|█████     | 50/100 [02:18<02:18,  2.77s/it]\u001b[A\n","[EVAL]:  51%|█████     | 51/100 [02:20<02:14,  2.75s/it]\u001b[A\n","[EVAL]:  52%|█████▏    | 52/100 [02:23<02:12,  2.76s/it]\u001b[A\n","[EVAL]:  53%|█████▎    | 53/100 [02:26<02:09,  2.75s/it]\u001b[A\n","[EVAL]:  54%|█████▍    | 54/100 [02:29<02:06,  2.75s/it]\u001b[A\n","[EVAL]:  55%|█████▌    | 55/100 [02:31<02:03,  2.74s/it]\u001b[A\n","[EVAL]:  56%|█████▌    | 56/100 [02:34<02:01,  2.76s/it]\u001b[A\n","[EVAL]:  57%|█████▋    | 57/100 [02:37<01:58,  2.75s/it]\u001b[A\n"]}],"source":["epochs = 30\n","batch_size = 128\n","num_layers = 2\n","learning_rate = 0.001\n","weight_decay = 0.0005\n","\n","num_layers = 2\n","num_heads = 6\n","dff = 4\n","train_dataloader, test_dataloader = create_dataloaders(batch_size)\n","\n","transformer = Transformer(num_layers=num_layers, d_model=256, num_heads=6, dff=4,\n","                          input_vocab_size=fr_vocab.n_words,\n","                          target_vocab_size=en_vocab.n_words)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n","\n","train(train_dataloader, transformer, optimizer, epochs)\n","\n"]},{"cell_type":"code","source":["transformer.save(f'layers{num_layers}_nh{num_heads}_dff{dff}.keras')"],"metadata":{"id":"ISKdO4Z-AX5j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"anx3WmiTW32A","executionInfo":{"status":"aborted","timestamp":1714499535070,"user_tz":300,"elapsed":10,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9j2nQ--1_AVq"},"source":["### Visualizing Attention\n","\n","A useful property of the attention mechanism is its highly interpretable\n","outputs. Because it is used to weight specific encoder outputs of the\n","input sequence, we can imagine looking where the network is focused most\n","at each time step.\n","\n","You could simply run ``plt.matshow(attentions)`` to see attention output\n","displayed as a matrix. For a better viewing experience we will do the\n","extra work of adding axes and labels:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1714499535070,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"gKlxYO0JTXzD"},"outputs":[],"source":["def plot_attention_head(in_tokens, translated_tokens, attention):\n","  # The model didn't generate `<START>` in the output. Skip it.\n","  translated_tokens = translated_tokens[1:]\n","\n","  ax = plt.gca()\n","  ax.matshow(attention)\n","  ax.set_xticks(range(len(in_tokens)))\n","  ax.set_yticks(range(len(translated_tokens)))\n","\n","  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n","  ax.set_xticklabels(\n","      labels, rotation=90)\n","\n","  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n","  ax.set_yticklabels(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1714499535070,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"9DIz0BzRcIh9"},"outputs":[],"source":["%matplotlib inline\n","import math\n","def showAttention(input_sentence, output_words, attentions):\n","    heads = attentions.shape[1]\n","    fig = plt.figure()\n","    fig, axs = plt.subplots(2,math.ceil(heads/2))\n","\n","    axs = axs.ravel()\n","    for i in range(heads):\n","\n","      axs[i].matshow(attentions[i], cmap='bone')\n","\n","      # Set up axes\n","      axs[i].set_xticklabels([''] + input_sentence.split(' '), rotation=90)\n","      axs[i].set_yticklabels([''] + output_words)\n","\n","      # # Show label at every tick\n","      # axs[i].xaxis.set_major_locator(ticker.MultipleLocator(1))\n","      # axs[i].yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","    plt.show()\n","\n","def evaluateAndShowAttention(input_sentence):\n","    output_sentence, _, _ = evaluate(transformer, input_sentence)\n","    attention_scores = transformer.decoder.last_attn_scores  # (batch, heads, target_seq, input_seq)\n","    print(\"=\"*30)\n","    print('input =', input_sentence)\n","    print('output =', output_sentence)\n","\n","    showAttention(input_sentence, output_sentence, attention_scores[0, : , :len(output_sentence), :])\n","\n","\n","evaluateAndShowAttention('python')\n","\n","evaluateAndShowAttention('je suis trop fatigue pour conduire')\n","\n","evaluateAndShowAttention('je suis desole si c est une question idiote')\n","\n","evaluateAndShowAttention('je suis reellement fiere de vous')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1714499535070,"user":{"displayName":"MICHAEL DENG","userId":"07422940239864829827"},"user_tz":300},"id":"rHE6JVWTgjvP"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}